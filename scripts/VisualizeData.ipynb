{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pose Prediction From Smartwatch Data\n",
    "\n",
    "Welcome, this notebook is the first chapter of the hackathon challenge to predict human arm pose from the sensor data of a single smartwatch. In other words, a human wears a smartwatch, and we will try to predict their arm pose from the sensor outputs we can read from the watch.\n",
    "\n",
    "## A Dive Into The Dataset\n",
    "\n",
    "In this first chapter, we will have a detailed look into the data and how to interpret it. You will also be presented with an in-depth Exploratory Data Analysis (EDA) process for covering various essential steps in data preprocessing. The training of predictive models comes afterward. The following sections guide you step-by-step through convenience functions for reading, plotting, normalizing and storing the data for future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Downloading Dependencies (Optional)\n",
    "\n",
    "We begin by downloading the libraries we will be using in this notebook. In case you have all of these pre-installed, you may skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install os\n",
    "!pip install matplotlib\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Importing Libraries\n",
    "We begin by importing the dependencies. The last import, aims to import the `config.py` file from the root directory of this project. This file simply stores path variables to where the data is located on your system. If you have not created the file yet, copy the `default_config.py` file in your project root, rename it into `config.py`, and adjust the \"data_path\" entry within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T18:14:08.595632105Z",
     "start_time": "2023-07-27T18:14:08.106699699Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import config       # adjust data paths in config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Raw Data\n",
    "\n",
    "Let's begin by loading the raw dataset into the notebook environment. You will notice multiple files in the data folder as the data was recorded in different instances over a period of a few days. Since all the attributes in the various files are the same, wouldn't it be better to have all of them in a single file? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T18:14:11.888505202Z",
     "start_time": "2023-07-27T18:14:08.639653132Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Loading the data set from the appropriate path and combining different files into one data frame\n",
    "\"\"\"\n",
    "\n",
    "path = config.paths[\"data_path\"]\n",
    "\n",
    "csv_files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
    "\n",
    "data_list = []\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_list.append(df)\n",
    "\n",
    "data_raw = pd.concat(data_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plotting Functions\n",
    "\n",
    "The goal of this chapter is helping you visualize and hint at identifying possible linkages that might show a strong or weak relationship between the different features (or columns) of the data. With the help of some modular functions, lets get a step closer to visualizing data.\n",
    "\n",
    "Bonus: As you progress through this notebook, we envourage you to use these functions for additional columns/features to interpret linkage between data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T18:14:08.639150797Z",
     "start_time": "2023-07-27T18:14:08.638499249Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This section represents all the functions required for various visualizations:\n",
    "        - The output_visualization function represents the X, Y, Z coordinates in a non-interactive graph.\n",
    "        - The interactive_output_visualization function helps in visualizing the X, Y, Z coordinates in an interactive 3D map. \n",
    "        - The distributionPlot function visualizes the 6 (or 1) different features. \n",
    "        - The correlationPlot function takes two features and graphically represents the correlation between them. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Helper function that provides appropriate the approriate length for columns.\n",
    "# The duration variable helps in defining the length of continuous data - this helps in visualizing a smaller hand trajectory \n",
    "# The randomseed variable helps to visualize the same data without randomizing the selection process\n",
    "# The remaining functions follow the same variables and analogy\n",
    "def data_columns(data, duration=None, randomseed=False):\n",
    "    dim1, dim2, dim3 = np.array(data['gt_hand_orig_rua_x']), np.array(data['gt_hand_orig_rua_y']), np.array(\n",
    "        data['gt_hand_orig_rua_z'])\n",
    "\n",
    "    if duration is not None:\n",
    "        if randomseed:\n",
    "            np.random.seed(42)\n",
    "        else:\n",
    "            np.random.seed(None)\n",
    "        reduced_samples = 10 * duration\n",
    "        start_index = np.random.choice(len(dim1) - reduced_samples + 1)\n",
    "        dim1 = dim1[start_index: start_index + reduced_samples]\n",
    "        dim2 = dim2[start_index: start_index + reduced_samples]\n",
    "        dim3 = dim3[start_index: start_index + reduced_samples]\n",
    "\n",
    "    return dim1, dim2, dim3\n",
    "\n",
    "\n",
    "def output_visualization(data, duration=None, randomseed=False):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    dim1, dim2, dim3 = data_columns(data)\n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.scatter(dim1, dim2, dim3, c='b', marker='.', s=1, alpha= 0.01)\n",
    "    ax.set_xlabel('X-Dimension')\n",
    "    ax.set_ylabel('Y-Dimension')\n",
    "    ax.set_zlabel('Z-Dimension')\n",
    "    ax.set_title('X-Y-Z Coordinates (all) in 3D Space')\n",
    "\n",
    "    dim1, dim2, dim3 = data_columns(data, duration, randomseed)\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.scatter(dim1, dim2, dim3, c='b', marker='o', s=5, alpha= 0.8)\n",
    "    ax.set_xlabel('X-Dimension')\n",
    "    ax.set_ylabel('Y-Dimension')\n",
    "    ax.set_zlabel('Z-Dimension')\n",
    "    ax.set_title('X-Y-Z Coordinates (some) in 3D Space')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def interactive_output_visualization(data, duration=None, randomseed=False):\n",
    "    if duration:\n",
    "        marker = dict(size=2, color='blue', opacity=0.8)\n",
    "    else:\n",
    "        marker = dict(size=1, color='blue', opacity=0.01)\n",
    "\n",
    "    \n",
    "    dim1, dim2, dim3 = data_columns(data, duration, randomseed)\n",
    "\n",
    "    trace = go.Scatter3d(x=dim1, y=dim2, z=dim3, mode='markers', marker=marker,\n",
    "                         name=\"Input Points\")\n",
    "    title = 'Values in 3D Space'\n",
    "    layout = go.Layout(title=title, scene=dict(xaxis=dict(title='X-Dimension'),\n",
    "                                               yaxis=dict(title='Y-Dimension'),\n",
    "                                               zaxis=dict(title='Z-Dimension')))\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def pltshow(x_lable, y_label, title, legend=False):\n",
    "    plt.xlabel(x_lable)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    if legend == True:\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def correlationPlot(data, label1=\"sw_pres_cal\", label2=\"gt_hand_orig_rua_y\"):\n",
    "    dim1, dim2 = np.array(data[label1]), np.array(data[label2])\n",
    "\n",
    "    slope, intercept = np.polyfit(dim1, dim2, deg=1)\n",
    "    regression_line = slope * dim1 + intercept\n",
    "\n",
    "    mse = np.mean((dim2 - regression_line) ** 2)\n",
    "\n",
    "    plt.scatter(dim1, dim2, c='b', marker='.', s=0.2, alpha=0.2)\n",
    "    plt.plot(dim1, regression_line, c='r', label='Linear Fit')\n",
    "    pltshow(f\"{label1}\", f\"{label2}\", f\"Correlation between {label1} & {label2}\\nMSE: {mse: .3f}\", legend=True)\n",
    "\n",
    "\n",
    "# The columns variable can be a list of strings (column names) or a string with a column name\n",
    "def distributionPlot(data,\n",
    "                     columns=[\"sw_6drr_cal_1\", 'sw_gyro_x', 'sw_lvel_x', \"sw_lacc_x\", 'sw_grav_x', 'sw_pres_cal']):\n",
    "    if type(columns) != str and len(columns) > 1:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(10, 5))\n",
    "\n",
    "        for i, column_name in enumerate(columns):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "\n",
    "            values = data[column_name]\n",
    "            mean_acceleration = np.mean(values)\n",
    "            std_acceleration = np.std(values)\n",
    "            axes[row, col].hist(values, bins='auto', density=True, alpha=0.4, color='b')\n",
    "            x = np.linspace(mean_acceleration - 3 * std_acceleration, mean_acceleration + 3 * std_acceleration, 100)\n",
    "            y = (1 / (std_acceleration * np.sqrt(2 * np.pi))) * np.exp(\n",
    "                -0.5 * ((x - mean_acceleration) / std_acceleration) ** 2)\n",
    "            axes[row, col].plot(x, y, color='r')\n",
    "            axes[row, col].set_xlabel(column_name)\n",
    "            axes[row, col].set_ylabel('Density')\n",
    "            axes[row, col].set_title('Distribution of ' + column_name)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        values = data[columns]\n",
    "\n",
    "        mean_acceleration = np.mean(values)\n",
    "        std_acceleration = np.std(values)\n",
    "        plt.hist(values, bins='auto', density=True, alpha=0.4, color='b')\n",
    "        x = np.linspace(mean_acceleration - 3 * std_acceleration, mean_acceleration + 3 * std_acceleration, 100)\n",
    "        y = (1 / (std_acceleration * np.sqrt(2 * np.pi))) * np.exp(\n",
    "            -0.5 * ((x - mean_acceleration) / std_acceleration) ** 2)\n",
    "        plt.plot(x, y, color='r')\n",
    "        pltshow(columns, 'Density', 'Distribution of ' + columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plots with Raw Data\n",
    "\n",
    "Time to see and interact with the data! The first plot (top-left) represents the entire X, Y & Z coordinates in the dataset where as the right plot represents a small trajectory of the hand movement. To make things more interesting, the next (bottom) plot allows you to interact in the 3D space - making much more sense on how the actual hand movement is be. Just click and drag on the plot. \n",
    "\n",
    "Note: We recommend uncommenting the 4 line in the following code block to visualize the 3 coordinates of the entire dataset. Since there are over 51,000 data points, it takes some time to display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T18:14:19.403201654Z",
     "start_time": "2023-07-27T18:14:11.889612663Z"
    }
   },
   "outputs": [],
   "source": [
    "duration = 20       # The duration variable can only have integer values. You may update to visualize different lengths of trajectories\n",
    "output_visualization(data_raw, duration=duration, randomseed=False)\n",
    "\n",
    "# interactive_output_visualization(data_raw)    # Uncomment this to view the entire X,Y,Z coordinates in the dataset\n",
    "interactive_output_visualization(data_raw, duration=duration, randomseed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe here that the raw data's distribution is almost centered around 0. This aids in assessing that the dataset adheres to certain statistical distributions. Can you notice some deviations and outliers in these graphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T18:14:33.551334015Z",
     "start_time": "2023-07-27T18:14:19.400214003Z"
    }
   },
   "outputs": [],
   "source": [
    "distributionPlot(data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following correlation graphs indicate that there might be a close linkage between pressure and ground truth hand origin (relative to the upper arm) Y. \n",
    "Conversely, we can notice that there seems to be no linkage at all between the smart watch's gravity and linear acceleration in the x directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T18:14:38.656637233Z",
     "start_time": "2023-07-27T18:14:33.550720647Z"
    }
   },
   "outputs": [],
   "source": [
    "correlationPlot(data_raw)\n",
    "correlationPlot(data_raw, label1=\"sw_grav_x\", label2=\"sw_lacc_x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Normalization\n",
    "\n",
    "This part normalizes the data and locally stores it together with means and standard deviations for future use. We normalize by subtracting the mean and dividing by the standard deviation. This newly transformed data is going to be of use in the next script, so we save it locally. \n",
    "\n",
    "In the end of this section, we check the data distribution plots of the normalized data and investigate the difference to distribution plots of the raw data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T18:24:49.854489961Z",
     "start_time": "2023-07-27T18:24:30.817689651Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gt_columns = [\n",
    "    \"gt_hand_orig_rua_x\", \"gt_hand_orig_rua_y\", \"gt_hand_orig_rua_z\",\n",
    "    \"gt_larm_6drr_rh_1\", \"gt_larm_6drr_rh_2\", \"gt_larm_6drr_rh_3\",\n",
    "    \"gt_larm_6drr_rh_4\", \"gt_larm_6drr_rh_5\", \"gt_larm_6drr_rh_6\",\n",
    "    \"gt_larm_orig_rua_x\", \"gt_larm_orig_rua_y\", \"gt_larm_orig_rua_z\",\n",
    "    \"gt_uarm_6drr_rh_1\", \"gt_uarm_6drr_rh_2\", \"gt_uarm_6drr_rh_3\",\n",
    "    \"gt_uarm_6drr_rh_4\", \"gt_uarm_6drr_rh_5\", \"gt_uarm_6drr_rh_6\"\n",
    "]\n",
    "\n",
    "sw_columns = [\n",
    "    \"sw_dt\",\n",
    "    \"sw_gyro_x\", \"sw_gyro_y\", \"sw_gyro_z\",\n",
    "    \"sw_lvel_x\", \"sw_lvel_y\", \"sw_lvel_z\",\n",
    "    \"sw_lacc_x\", \"sw_lacc_y\", \"sw_lacc_z\",\n",
    "    \"sw_grav_x\", \"sw_grav_y\", \"sw_grav_z\",\n",
    "    \"sw_6drr_cal_1\", \"sw_6drr_cal_2\", \"sw_6drr_cal_3\",\n",
    "    \"sw_6drr_cal_4\", \"sw_6drr_cal_5\", \"sw_6drr_cal_6\",\n",
    "    \"sw_pres_cal\"\n",
    "]\n",
    "\n",
    "# filter the columns of interest from the raw data file\n",
    "data_raw_cs = data_raw.loc[:, gt_columns + sw_columns]\n",
    "\n",
    "# get means and std of every column\n",
    "raw_means = data_raw_cs.mean()\n",
    "raw_stds = data_raw_cs.std()\n",
    "\n",
    "# we normalize by subtracting the mean and dividing by the std\n",
    "normalized_data = (data_raw_cs - raw_means) / raw_stds\n",
    "\n",
    "# combine mean and std into one DataFrame\n",
    "means_stds_df = pd.concat([raw_means, raw_stds], axis=1)\n",
    "\n",
    "# create the cache directory if it doesn't exist yet\n",
    "cache_path = config.paths[\"cache_path\"]\n",
    "if not cache_path.exists():\n",
    "    cache_path.mkdir(parents=True)\n",
    "\n",
    "# store normalized data as well as created means and stds for later use during training and testing\n",
    "normalized_data.to_csv(cache_path / 'normalized_data.csv', index=False)\n",
    "means_stds_df.to_csv(cache_path / 'means_stds.csv', index_label='Column')\n",
    "print(f\"Stored normalized data to: {cache_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have normalized the data and stored it into the cache directory in your project root. Let us have a look at the distribution plots again. The scales of the data should have changed and distinct data columns should now be in similar value ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributionPlot(normalized_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
