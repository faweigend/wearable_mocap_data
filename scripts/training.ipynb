{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Prediction From Smartwatch Data\n",
    "### Part 2\n",
    "\n",
    "Welcome! This notebook is the second chapter of the hackathon challenge to predict human arm pose from the sensor data of a single smartwatch. In other words, a human wears a smartwatch, and we will try to predict their arm pose from the sensor outputs we can read from the watch.\n",
    "\n",
    "## Creating a Predictive Model with Machine Learning\n",
    "\n",
    "In this second chapter, we will have the opportunity to utilize the processed data from the previous chapter and train a Neural Network to generate predictions. The following sections will guide you step-by-step through model architecture design, model training and evaluation, performance analysis, and 3D visualization for better interpretability of the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Downloading Dependencies (Optional)\n",
    "\n",
    "We begin by downloading the libraries in this notebook. If you have all of these pre-installed, you may skip this step.\n",
    "\n",
    "Note: We installed some packages in the first notebook. If you skipped running it, please run step 0 from the first notebook to download dependacies and continute with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/affanbinusman/Library/Python/3.9/lib/python/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/affanbinusman/Library/Python/3.9/lib/python/site-packages (from torch) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/affanbinusman/Library/Python/3.9/lib/python/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/affanbinusman/Library/Python/3.9/lib/python/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/affanbinusman/Library/Python/3.9/lib/python/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/affanbinusman/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/affanbinusman/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/affanbinusman/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /Users/affanbinusman/Library/Python/3.9/lib/python/site-packages (0.0.post1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries\n",
    "\n",
    "Lets import the libraries we will be primarily working with in this notebook. You may notice some similar ones. Let us help you understand why:\n",
    "- Libraries like Numpy and Pandas are helpful for mathematical functions and data frameworks respectively. Therefore, you would see them almost everywhere.\n",
    "- Matplotlib is often used to display plots. Using plotly, we will display interactive plots. But why do we need plots for training a machine learning model? You will find out soon. \n",
    "- Finally, Torch (PyTorch) provides powerful tools to deploy Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Libraries for Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split    # Scikit-Lean for preparing the train-test data split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# adjust data paths in config.py\n",
    "import config   \n",
    "import os    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Processed Data\n",
    "\n",
    "Let's load the normalized dataset into the environment that we processed in the previous notebook. You will notice two files in the cache folder. \n",
    "1. `normalized_data.csv` contains the normalized dataset\n",
    "2. `means_stds.csv` contains the means and standard deviations of different columns utilized during normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = config.paths[\"cache_path\"]\n",
    "path = os.path.join(cache_path, \"normalized_data.csv\")\n",
    "data_normalized = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plotting Functions\n",
    "Lets get the simpler stuff out of the way first. Since the neural networks will help estimate the X, Y & Z coordinates, we use a similar 3D visualization for representing the outputs by the model and actual labels to gain a deeper understanding of model's performance. \n",
    "\n",
    "Secondly, during the training process you will notice the loss decreases with epochs. It is useful to visualize the loss for a number of reasons:\n",
    "- Overfitting: When model starts remembering the training dataset instead of figuring out a \"pattern\".\n",
    "- Performance: To figure out if further training decreases or increases loss \n",
    "\n",
    "Note: Epoch refers to the iteration number where the model goes through the entire training data. For instance, 1 epoch indicates that model has passed through the data once. \n",
    "\n",
    "For these purposes, we are using helper functions that can be called repeatedly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This section represents the function and helper function required for interactive 3D visualization:\n",
    "        - The interactive_output_visualization function helps in visualizing the X, Y, Z coordinates in an interactive 3D map. It is also used to display the output and original labels\n",
    "\"\"\"\n",
    "\n",
    "# Helper function that provides appropriate the approriate length for columns. \n",
    "# The duration variable helps in defining the length of continuous data - this helps in visualizing a smaller hand trajectory \n",
    "# The randomseed variable helps to visualize the same data without randomizing the selection process\n",
    "# The remaining functions follow the same variables and analogy\n",
    "def data_visualization(data, duration, randomseed=False, output=False):\n",
    "    if output is True:\n",
    "        dim1, dim2, dim3 = data[:,0], data[:,1], data[:,2]\n",
    "    else:\n",
    "        dim1, dim2, dim3 = np.array(data['gt_hand_orig_rua_x']), np.array(data['gt_hand_orig_rua_y']), np.array(data['gt_hand_orig_rua_z'])\n",
    "\n",
    "    if duration is not None:\n",
    "        if randomseed:\n",
    "            np.random.seed(42)\n",
    "        else: \n",
    "            np.random.seed(None)\n",
    "        reduced_samples = 10 * duration\n",
    "        start_index = np.random.choice(len(dim1) - reduced_samples + 1)\n",
    "        dim1 = dim1[start_index : start_index + reduced_samples]\n",
    "        dim2 = dim2[start_index : start_index + reduced_samples]\n",
    "        dim3 = dim3[start_index : start_index + reduced_samples]\n",
    "        \n",
    "    return dim1, dim2, dim3\n",
    "\n",
    "def interactive_output_visualization(data, duration=None, randomseed=False, output=False, output_data = None):\n",
    "    dim1, dim2, dim3 = data_visualization(data, duration, randomseed, output)\n",
    "    \n",
    "    trace = go.Scatter3d(x=dim1, y=dim2, z=dim3, mode='markers', marker=dict(size=3, \n",
    "                                                                             color='blue', \n",
    "                                                                             opacity=0.8),\n",
    "                                                                             name=\"Input Points\")\n",
    "\n",
    "    if output:\n",
    "        dim1, dim2, dim3 = data_visualization(output_data, duration, randomseed, output)\n",
    "        trace_output = go.Scatter3d(x=dim1, y=dim2, z=dim3, mode='markers', marker=dict(size=3, \n",
    "                                                                                    color='red', \n",
    "                                                                                    opacity=0.5), \n",
    "                                                                                    name='Output Points')\n",
    "        traces = [trace, trace_output]\n",
    "        title = 'Input and Output Points in 3D Space'\n",
    "    else:\n",
    "        traces = [trace]\n",
    "        title = 'Values in 3D Space'\n",
    "\n",
    "    layout = go.Layout(title=title, scene=dict(xaxis=dict(title='X-Dimension'),\n",
    "                                                                        yaxis=dict(title='Y-Dimension'),\n",
    "                                                                        zaxis=dict(title='Z-Dimension')))\n",
    "\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "def lossPlot(losses):\n",
    "    epochs_total = range(1, 1 + len(losses[\"evaluation\"]))\n",
    "\n",
    "    plt.subplots(1, figsize=(20,5))\n",
    "    plt.plot(epochs_total, losses[\"training\"], label = \"Training Loss\")\n",
    "    plt.plot(epochs_total, losses[\"evaluation\"], label = \"Evaluation Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Losses over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Loader & Model Architecture\n",
    "\n",
    "A data loader is an integral part of training a neural network. The data loader assists in selecting the relevant columns assigned as features and labels. We accomplish this task using the PyTorch library. \n",
    "\n",
    "In the next portion of the code block, we implement a basic feed-forward architecture comprising an input, two hidden, and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Data Loader: \n",
    "        This serves as the framework to load data using pandas and segment out the useful columns (labels and inputs)\n",
    "\"\"\"\n",
    "gt_columns = [\n",
    "    \"gt_hand_orig_rua_x\", \"gt_hand_orig_rua_y\", \"gt_hand_orig_rua_z\",\n",
    "    \"gt_larm_6drr_rh_1\", \"gt_larm_6drr_rh_2\", \"gt_larm_6drr_rh_3\",\n",
    "    \"gt_larm_6drr_rh_4\", \"gt_larm_6drr_rh_5\", \"gt_larm_6drr_rh_6\",\n",
    "    \"gt_larm_orig_rua_x\", \"gt_larm_orig_rua_y\", \"gt_larm_orig_rua_z\",\n",
    "    \"gt_uarm_6drr_rh_1\", \"gt_uarm_6drr_rh_2\", \"gt_uarm_6drr_rh_3\",\n",
    "    \"gt_uarm_6drr_rh_4\", \"gt_uarm_6drr_rh_5\", \"gt_uarm_6drr_rh_6\"\n",
    "]\n",
    "\n",
    "sw_columns = [\n",
    "    \"sw_dt\",\n",
    "    \"sw_gyro_x\", \"sw_gyro_y\", \"sw_gyro_z\",\n",
    "    \"sw_lvel_x\", \"sw_lvel_y\", \"sw_lvel_z\",\n",
    "    \"sw_lacc_x\", \"sw_lacc_y\", \"sw_lacc_z\",\n",
    "    \"sw_grav_x\", \"sw_grav_y\", \"sw_grav_z\",\n",
    "    \"sw_6drr_cal_1\", \"sw_6drr_cal_2\", \"sw_6drr_cal_3\",\n",
    "    \"sw_6drr_cal_4\", \"sw_6drr_cal_5\", \"sw_6drr_cal_6\",\n",
    "    \"sw_pres_cal\"\n",
    "]\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__ (self, data):\n",
    "        self.data = data\n",
    "        self.features = self.data.loc[:, sw_columns].values     # All smart watch inputs that contribute towards the outputs\n",
    "        self.labels = self.data.loc[:, gt_columns[0:3]].values          # The model predicts X, Y, Z coordinates of hand\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32) \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return features, label\n",
    "    \n",
    "\"\"\"\n",
    "    Neural Network / Model:\n",
    "        A simple linear architecture based model with 3 layers (including input and output layers). \n",
    "\"\"\"\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__ (self, input_size, hidden_size, output_size):\n",
    "        super (MyModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size + int(hidden_size/2))\n",
    "        self.fc2 = nn.Linear(hidden_size + int(hidden_size/2), hidden_size - int(hidden_size/2))\n",
    "        self.fc3 = nn.Linear(hidden_size - int(hidden_size/2), output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block provides a basic overview of some of the tunable parameters. We encourage you to play around with these parameters to see what works better for this model architecture and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000        \n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "test_data_size = 0.2            # Range 0-1. You may increase or decrease the recommended testing data size by updating this variable\n",
    "hidden_size = 128               # This represents the middle layer of the neural network. You may manually play around with the model architecture by adding more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our model with the help of helper functions defined previously. You would also notice the `criterion` and `optimizer` variables that work differently for different tasks. Feel free to play around with them. \n",
    "\n",
    "For more information on loss functions and optimizers, visit:  \n",
    "- https://pytorch.org/docs/stable/nn.html#loss-functions     \n",
    "- https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (fc1): Linear(in_features=20, out_features=192, bias=True)\n",
       "  (fc2): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loading calls to class and functions\n",
    "dataset = MyDataset(data_normalized)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=test_data_size, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialzing the model with inputs\n",
    "input_size = len(dataset.features[0])\n",
    "output_size = len(dataset.labels[0])\n",
    "\n",
    "model = MyModel(input_size, hidden_size, output_size)\n",
    "# Try out different loss functions and optimizers to see what works with different tasks. \n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.ASGD(model.parameters(), lr=learning_rate)     \n",
    "\n",
    "\"\"\"\n",
    "    If you have GPU resource available, the training process will be faster!\n",
    "    It's okay if you dont have a GPU. The code works without one as well. \n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training & Evaluation\n",
    "\n",
    "Let's train our data using the defined neural network. The notebook implements the training loop, where batches of data are fed into the model iteratively. During training, the model learns to optimize its parameters to minimize the defined loss function through backpropagation. Training hyperparameters, such as learning rate, batch size, and number of epochs, can be fine-tuned to achieve optimal performance.\n",
    "\n",
    "Following training, you will notice the evaluation block that evaluates the neural network's performance on a separate test dataset (not included in training) to assess its performance. \n",
    "\n",
    "Lastly, the `losses` dictionary is populated to visualize important metrics later in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Training Loss: 0.4156285114766319, Smooth L1 Loss: 0.4025159035451137\n",
      "Epoch 2/500, Training Loss: 0.3977817245151686, Smooth L1 Loss: 0.38521174685313153\n",
      "Epoch 3/500, Training Loss: 0.37919449863802407, Smooth L1 Loss: 0.36633145365004355\n",
      "Epoch 4/500, Training Loss: 0.35889011275941046, Smooth L1 Loss: 0.34537629482264703\n",
      "Epoch 5/500, Training Loss: 0.33674079633277393, Smooth L1 Loss: 0.32246481713194114\n",
      "Epoch 6/500, Training Loss: 0.31311856649348124, Smooth L1 Loss: 0.298234349116683\n",
      "Epoch 7/500, Training Loss: 0.28894921525377004, Smooth L1 Loss: 0.2738981438227571\n",
      "Epoch 8/500, Training Loss: 0.26560688364333, Smooth L1 Loss: 0.2510418323799968\n",
      "Epoch 9/500, Training Loss: 0.24446676520333774, Smooth L1 Loss: 0.2307637078114427\n",
      "Epoch 10/500, Training Loss: 0.22617310191986065, Smooth L1 Loss: 0.21337512063865477\n",
      "Epoch 11/500, Training Loss: 0.21069069980567204, Smooth L1 Loss: 0.19868825741398793\n",
      "Epoch 12/500, Training Loss: 0.1975797124314999, Smooth L1 Loss: 0.18628238736150357\n",
      "Epoch 13/500, Training Loss: 0.18639135637864973, Smooth L1 Loss: 0.17573041232446066\n",
      "Epoch 14/500, Training Loss: 0.17670310241876594, Smooth L1 Loss: 0.16666156608754626\n",
      "Epoch 15/500, Training Loss: 0.1681579240926222, Smooth L1 Loss: 0.1588068539276719\n",
      "Epoch 16/500, Training Loss: 0.16060530055980177, Smooth L1 Loss: 0.15194405987858772\n",
      "Epoch 17/500, Training Loss: 0.153823577011553, Smooth L1 Loss: 0.14588392031593964\n",
      "Epoch 18/500, Training Loss: 0.14780625121461022, Smooth L1 Loss: 0.140512156407707\n",
      "Epoch 19/500, Training Loss: 0.14238416684278543, Smooth L1 Loss: 0.13573448961743942\n",
      "Epoch 20/500, Training Loss: 0.13755010962846198, Smooth L1 Loss: 0.1314519649872986\n",
      "Epoch 21/500, Training Loss: 0.13319541855854689, Smooth L1 Loss: 0.12763035093219235\n",
      "Epoch 22/500, Training Loss: 0.1293263407289118, Smooth L1 Loss: 0.12414845989014094\n",
      "Epoch 23/500, Training Loss: 0.12586657854094022, Smooth L1 Loss: 0.12106224024095215\n",
      "Epoch 24/500, Training Loss: 0.12276457894395515, Smooth L1 Loss: 0.11822592635424091\n",
      "Epoch 25/500, Training Loss: 0.11999585243742823, Smooth L1 Loss: 0.11570220666292769\n",
      "Epoch 26/500, Training Loss: 0.11752927737031582, Smooth L1 Loss: 0.1134113734229826\n",
      "Epoch 27/500, Training Loss: 0.115303925468006, Smooth L1 Loss: 0.11135490636269633\n",
      "Epoch 28/500, Training Loss: 0.11332328288235526, Smooth L1 Loss: 0.10950940933365089\n",
      "Epoch 29/500, Training Loss: 0.11155588883492681, Smooth L1 Loss: 0.1078114673280372\n",
      "Epoch 30/500, Training Loss: 0.10996022140202315, Smooth L1 Loss: 0.10629398565596113\n",
      "Epoch 31/500, Training Loss: 0.10851457760025914, Smooth L1 Loss: 0.10492986859753728\n",
      "Epoch 32/500, Training Loss: 0.10720684915637048, Smooth L1 Loss: 0.10367316304920958\n",
      "Epoch 33/500, Training Loss: 0.10603205892054932, Smooth L1 Loss: 0.10253600156507812\n",
      "Epoch 34/500, Training Loss: 0.10496587115497405, Smooth L1 Loss: 0.10152121250016186\n",
      "Epoch 35/500, Training Loss: 0.10397469965012177, Smooth L1 Loss: 0.10058865502763253\n",
      "Epoch 36/500, Training Loss: 0.10306863114237785, Smooth L1 Loss: 0.0997203430519081\n",
      "Epoch 37/500, Training Loss: 0.10224183612831549, Smooth L1 Loss: 0.09895199666229579\n",
      "Epoch 38/500, Training Loss: 0.10147584130726575, Smooth L1 Loss: 0.09818155483271067\n",
      "Epoch 39/500, Training Loss: 0.10076430017029606, Smooth L1 Loss: 0.0975303234030994\n",
      "Epoch 40/500, Training Loss: 0.10007694863898743, Smooth L1 Loss: 0.09691029428862609\n",
      "Epoch 41/500, Training Loss: 0.09945575052485374, Smooth L1 Loss: 0.09629580224505983\n",
      "Epoch 42/500, Training Loss: 0.09886945798489206, Smooth L1 Loss: 0.095720218315434\n",
      "Epoch 43/500, Training Loss: 0.09830141468828427, Smooth L1 Loss: 0.09521612714832792\n",
      "Epoch 44/500, Training Loss: 0.09777547917576228, Smooth L1 Loss: 0.09474276733369781\n",
      "Epoch 45/500, Training Loss: 0.09727543734625918, Smooth L1 Loss: 0.09426558322201554\n",
      "Epoch 46/500, Training Loss: 0.09679963079771557, Smooth L1 Loss: 0.09385174007799763\n",
      "Epoch 47/500, Training Loss: 0.09635640664593033, Smooth L1 Loss: 0.09340782420566449\n",
      "Epoch 48/500, Training Loss: 0.09590496977674212, Smooth L1 Loss: 0.09301271783904387\n",
      "Epoch 49/500, Training Loss: 0.09549919595033074, Smooth L1 Loss: 0.09263607293653947\n",
      "Epoch 50/500, Training Loss: 0.09508930704587899, Smooth L1 Loss: 0.09227331974901833\n",
      "Epoch 51/500, Training Loss: 0.09470507504355505, Smooth L1 Loss: 0.09192028129473329\n",
      "Epoch 52/500, Training Loss: 0.09433320943932026, Smooth L1 Loss: 0.09162026815689527\n",
      "Epoch 53/500, Training Loss: 0.09399782670076919, Smooth L1 Loss: 0.09128868081965126\n",
      "Epoch 54/500, Training Loss: 0.09364284440011218, Smooth L1 Loss: 0.09094954307119434\n",
      "Epoch 55/500, Training Loss: 0.09331314962195314, Smooth L1 Loss: 0.09068529429630591\n",
      "Epoch 56/500, Training Loss: 0.09299696602179232, Smooth L1 Loss: 0.09039449519835986\n",
      "Epoch 57/500, Training Loss: 0.09268172641379246, Smooth L1 Loss: 0.09010601670552905\n",
      "Epoch 58/500, Training Loss: 0.09237857988995055, Smooth L1 Loss: 0.08986293395551351\n",
      "Epoch 59/500, Training Loss: 0.09207389908640282, Smooth L1 Loss: 0.08958358220899335\n",
      "Epoch 60/500, Training Loss: 0.09178879849864664, Smooth L1 Loss: 0.089327357781048\n",
      "Epoch 61/500, Training Loss: 0.09151713190159359, Smooth L1 Loss: 0.08910478948830412\n",
      "Epoch 62/500, Training Loss: 0.09125761808332614, Smooth L1 Loss: 0.08888265340087506\n",
      "Epoch 63/500, Training Loss: 0.09100319929240983, Smooth L1 Loss: 0.08866949014079112\n",
      "Epoch 64/500, Training Loss: 0.09073869374711156, Smooth L1 Loss: 0.08843889841451667\n",
      "Epoch 65/500, Training Loss: 0.09048661864970041, Smooth L1 Loss: 0.0882124481901813\n",
      "Epoch 66/500, Training Loss: 0.09025454792927429, Smooth L1 Loss: 0.08800803094457549\n",
      "Epoch 67/500, Training Loss: 0.09001802858235179, Smooth L1 Loss: 0.08778633279367708\n",
      "Epoch 68/500, Training Loss: 0.08978456881455177, Smooth L1 Loss: 0.08759088130094685\n",
      "Epoch 69/500, Training Loss: 0.08957300963249183, Smooth L1 Loss: 0.08746596882477976\n",
      "Epoch 70/500, Training Loss: 0.08936072662832656, Smooth L1 Loss: 0.08724765784035508\n",
      "Epoch 71/500, Training Loss: 0.08914196867369799, Smooth L1 Loss: 0.08704931494923165\n",
      "Epoch 72/500, Training Loss: 0.08893199175020347, Smooth L1 Loss: 0.08687856823850709\n",
      "Epoch 73/500, Training Loss: 0.08872860144589834, Smooth L1 Loss: 0.08670921380130145\n",
      "Epoch 74/500, Training Loss: 0.08853518361774619, Smooth L1 Loss: 0.08653350018609601\n",
      "Epoch 75/500, Training Loss: 0.08833828795215358, Smooth L1 Loss: 0.0864231252612976\n",
      "Epoch 76/500, Training Loss: 0.08816403013784528, Smooth L1 Loss: 0.08626422053202987\n",
      "Epoch 77/500, Training Loss: 0.08796929447475263, Smooth L1 Loss: 0.08610032711966106\n",
      "Epoch 78/500, Training Loss: 0.08779033516412196, Smooth L1 Loss: 0.08594482137749974\n",
      "Epoch 79/500, Training Loss: 0.08761335682609807, Smooth L1 Loss: 0.08582766060359202\n",
      "Epoch 80/500, Training Loss: 0.08745061861720062, Smooth L1 Loss: 0.08569060638546944\n",
      "Epoch 81/500, Training Loss: 0.08727629349571495, Smooth L1 Loss: 0.08553077814240868\n",
      "Epoch 82/500, Training Loss: 0.08710874354349818, Smooth L1 Loss: 0.08540004276885436\n",
      "Epoch 83/500, Training Loss: 0.08694336181821455, Smooth L1 Loss: 0.08527829259848939\n",
      "Epoch 84/500, Training Loss: 0.0867956468772485, Smooth L1 Loss: 0.085145935917703\n",
      "Epoch 85/500, Training Loss: 0.08663995090674087, Smooth L1 Loss: 0.08502940090301518\n",
      "Epoch 86/500, Training Loss: 0.08648246868652998, Smooth L1 Loss: 0.08487928395446104\n",
      "Epoch 87/500, Training Loss: 0.08634035512445054, Smooth L1 Loss: 0.08476651130387416\n",
      "Epoch 88/500, Training Loss: 0.08619093693396895, Smooth L1 Loss: 0.08469345490448177\n",
      "Epoch 89/500, Training Loss: 0.08604041240425502, Smooth L1 Loss: 0.08452958694229332\n",
      "Epoch 90/500, Training Loss: 0.08590273742658505, Smooth L1 Loss: 0.08441574586770283\n",
      "Epoch 91/500, Training Loss: 0.08575874540037003, Smooth L1 Loss: 0.08433407093756475\n",
      "Epoch 92/500, Training Loss: 0.08563084027545463, Smooth L1 Loss: 0.08421288038460681\n",
      "Epoch 93/500, Training Loss: 0.08550044212148385, Smooth L1 Loss: 0.0841088281192172\n",
      "Epoch 94/500, Training Loss: 0.08537396270295848, Smooth L1 Loss: 0.08398000448225783\n",
      "Epoch 95/500, Training Loss: 0.08524170454933448, Smooth L1 Loss: 0.0839355095791129\n",
      "Epoch 96/500, Training Loss: 0.08512053530717241, Smooth L1 Loss: 0.08382259687193884\n",
      "Epoch 97/500, Training Loss: 0.08499840650581507, Smooth L1 Loss: 0.08374298367506036\n",
      "Epoch 98/500, Training Loss: 0.0848662229479799, Smooth L1 Loss: 0.08364492629726346\n",
      "Epoch 99/500, Training Loss: 0.0847511797927428, Smooth L1 Loss: 0.08354221830646005\n",
      "Epoch 100/500, Training Loss: 0.08462589543654724, Smooth L1 Loss: 0.08346637402876066\n",
      "Epoch 101/500, Training Loss: 0.08450441407984581, Smooth L1 Loss: 0.08336568801090695\n",
      "Epoch 102/500, Training Loss: 0.08441656406806863, Smooth L1 Loss: 0.08331460740345602\n",
      "Epoch 103/500, Training Loss: 0.08428620507463741, Smooth L1 Loss: 0.08321763696865393\n",
      "Epoch 104/500, Training Loss: 0.08417656285247364, Smooth L1 Loss: 0.08312677241002138\n",
      "Epoch 105/500, Training Loss: 0.08406301429450223, Smooth L1 Loss: 0.08303599417782746\n",
      "Epoch 106/500, Training Loss: 0.0839609257372969, Smooth L1 Loss: 0.08295051542182381\n",
      "Epoch 107/500, Training Loss: 0.08385320013200027, Smooth L1 Loss: 0.0828707091546116\n",
      "Epoch 108/500, Training Loss: 0.08374475480781661, Smooth L1 Loss: 0.0827913570146148\n",
      "Epoch 109/500, Training Loss: 0.08364825602170926, Smooth L1 Loss: 0.08271720201278535\n",
      "Epoch 110/500, Training Loss: 0.08355622387666634, Smooth L1 Loss: 0.08267738281462628\n",
      "Epoch 111/500, Training Loss: 0.08343903740175104, Smooth L1 Loss: 0.08258407048952694\n",
      "Epoch 112/500, Training Loss: 0.08334866527845894, Smooth L1 Loss: 0.08251776342065288\n",
      "Epoch 113/500, Training Loss: 0.08324956872325012, Smooth L1 Loss: 0.08242122372254156\n",
      "Epoch 114/500, Training Loss: 0.08315026254397659, Smooth L1 Loss: 0.08236846444196999\n",
      "Epoch 115/500, Training Loss: 0.08306313495057216, Smooth L1 Loss: 0.08230115627296843\n",
      "Epoch 116/500, Training Loss: 0.08297523674397653, Smooth L1 Loss: 0.08220578491902696\n",
      "Epoch 117/500, Training Loss: 0.08287601630036957, Smooth L1 Loss: 0.08216410060413182\n",
      "Epoch 118/500, Training Loss: 0.08279072396133257, Smooth L1 Loss: 0.08211106814157504\n",
      "Epoch 119/500, Training Loss: 0.0827031424763986, Smooth L1 Loss: 0.08205917078213623\n",
      "Epoch 120/500, Training Loss: 0.08261767600253585, Smooth L1 Loss: 0.08198340351764973\n",
      "Epoch 121/500, Training Loss: 0.08252113202703748, Smooth L1 Loss: 0.08189905596037324\n",
      "Epoch 122/500, Training Loss: 0.082444341160393, Smooth L1 Loss: 0.08185771447964586\n",
      "Epoch 123/500, Training Loss: 0.08235775153850011, Smooth L1 Loss: 0.08179489865254325\n",
      "Epoch 124/500, Training Loss: 0.08227456724586119, Smooth L1 Loss: 0.08175150781440046\n",
      "Epoch 125/500, Training Loss: 0.0822069760106036, Smooth L1 Loss: 0.08169502344054098\n",
      "Epoch 126/500, Training Loss: 0.08212507373972791, Smooth L1 Loss: 0.0816239515820948\n",
      "Epoch 127/500, Training Loss: 0.08203864255965045, Smooth L1 Loss: 0.08158749474499088\n",
      "Epoch 128/500, Training Loss: 0.08194465110987281, Smooth L1 Loss: 0.08148412483457762\n",
      "Epoch 129/500, Training Loss: 0.08186632580155336, Smooth L1 Loss: 0.0814762122463435\n",
      "Epoch 130/500, Training Loss: 0.08179567878444989, Smooth L1 Loss: 0.08141240479352956\n",
      "Epoch 131/500, Training Loss: 0.08173192418427859, Smooth L1 Loss: 0.08135106353662334\n",
      "Epoch 132/500, Training Loss: 0.08165364438928843, Smooth L1 Loss: 0.08130358127303995\n",
      "Epoch 133/500, Training Loss: 0.08156873760879904, Smooth L1 Loss: 0.08125225397256705\n",
      "Epoch 134/500, Training Loss: 0.0815064567838602, Smooth L1 Loss: 0.08122762729628728\n",
      "Epoch 135/500, Training Loss: 0.08143053140833183, Smooth L1 Loss: 0.08121126220346643\n",
      "Epoch 136/500, Training Loss: 0.08135158372458053, Smooth L1 Loss: 0.08112081011327413\n",
      "Epoch 137/500, Training Loss: 0.08127942575132789, Smooth L1 Loss: 0.08106147379686053\n",
      "Epoch 138/500, Training Loss: 0.08120555031126823, Smooth L1 Loss: 0.08102685938445994\n",
      "Epoch 139/500, Training Loss: 0.08115312953790028, Smooth L1 Loss: 0.08100997088835217\n",
      "Epoch 140/500, Training Loss: 0.08107870306081819, Smooth L1 Loss: 0.08093130212420455\n",
      "Epoch 141/500, Training Loss: 0.08101474254387589, Smooth L1 Loss: 0.08089877935484625\n",
      "Epoch 142/500, Training Loss: 0.08094849226915318, Smooth L1 Loss: 0.08085644608721711\n",
      "Epoch 143/500, Training Loss: 0.08087235476803664, Smooth L1 Loss: 0.08085935989108223\n",
      "Epoch 144/500, Training Loss: 0.08081395997877282, Smooth L1 Loss: 0.08080196052861328\n",
      "Epoch 145/500, Training Loss: 0.08073466030007975, Smooth L1 Loss: 0.08074551020175792\n",
      "Epoch 146/500, Training Loss: 0.0806828607323665, Smooth L1 Loss: 0.08068264741450548\n",
      "Epoch 147/500, Training Loss: 0.08062462184740149, Smooth L1 Loss: 0.08066691630161725\n",
      "Epoch 148/500, Training Loss: 0.08055794431607505, Smooth L1 Loss: 0.08062910311855376\n",
      "Epoch 149/500, Training Loss: 0.08049279436972982, Smooth L1 Loss: 0.08056074500871965\n",
      "Epoch 150/500, Training Loss: 0.08043178057541019, Smooth L1 Loss: 0.08050961955450475\n",
      "Epoch 151/500, Training Loss: 0.0803738278179353, Smooth L1 Loss: 0.08048906782642007\n",
      "Epoch 152/500, Training Loss: 0.08030778801311618, Smooth L1 Loss: 0.08044742685384475\n",
      "Epoch 153/500, Training Loss: 0.08024829973417204, Smooth L1 Loss: 0.08043514285236597\n",
      "Epoch 154/500, Training Loss: 0.08018742762685974, Smooth L1 Loss: 0.08041925701456001\n",
      "Epoch 155/500, Training Loss: 0.08013026320919898, Smooth L1 Loss: 0.08037187737556031\n",
      "Epoch 156/500, Training Loss: 0.0800859099029055, Smooth L1 Loss: 0.08033978568318371\n",
      "Epoch 157/500, Training Loss: 0.08002663648502838, Smooth L1 Loss: 0.08029658026778354\n",
      "Epoch 158/500, Training Loss: 0.0799682736756721, Smooth L1 Loss: 0.08025535197856908\n",
      "Epoch 159/500, Training Loss: 0.07990455857797521, Smooth L1 Loss: 0.08024858934088396\n",
      "Epoch 160/500, Training Loss: 0.07985394084511171, Smooth L1 Loss: 0.08018250790281364\n",
      "Epoch 161/500, Training Loss: 0.07979505615317879, Smooth L1 Loss: 0.08015514691718496\n",
      "Epoch 162/500, Training Loss: 0.07974066192977095, Smooth L1 Loss: 0.08011406330535045\n",
      "Epoch 163/500, Training Loss: 0.07969228028891168, Smooth L1 Loss: 0.08008054049255756\n",
      "Epoch 164/500, Training Loss: 0.07963157761931995, Smooth L1 Loss: 0.08001287093457694\n",
      "Epoch 165/500, Training Loss: 0.07959570354165663, Smooth L1 Loss: 0.08002365029488619\n",
      "Epoch 166/500, Training Loss: 0.07954315376886423, Smooth L1 Loss: 0.08002683265994374\n",
      "Epoch 167/500, Training Loss: 0.07947844524242452, Smooth L1 Loss: 0.07996068593974297\n",
      "Epoch 168/500, Training Loss: 0.07943885753624105, Smooth L1 Loss: 0.07993418789611986\n",
      "Epoch 169/500, Training Loss: 0.07938080543769155, Smooth L1 Loss: 0.07991227688482748\n",
      "Epoch 170/500, Training Loss: 0.07933179326895354, Smooth L1 Loss: 0.07991245116751927\n",
      "Epoch 171/500, Training Loss: 0.07927517529918952, Smooth L1 Loss: 0.07982419843140703\n",
      "Epoch 172/500, Training Loss: 0.07923590064336712, Smooth L1 Loss: 0.0798112080217554\n",
      "Epoch 173/500, Training Loss: 0.07918675123277494, Smooth L1 Loss: 0.07981512349886963\n",
      "Epoch 174/500, Training Loss: 0.07912530175945609, Smooth L1 Loss: 0.07977187828733943\n",
      "Epoch 175/500, Training Loss: 0.0790875088153542, Smooth L1 Loss: 0.0797556782535349\n",
      "Epoch 176/500, Training Loss: 0.0790466313128886, Smooth L1 Loss: 0.07971772601684698\n",
      "Epoch 177/500, Training Loss: 0.07899431992268217, Smooth L1 Loss: 0.07965432444157508\n",
      "Epoch 178/500, Training Loss: 0.07894488832584902, Smooth L1 Loss: 0.07963464169118267\n",
      "Epoch 179/500, Training Loss: 0.07889640840571284, Smooth L1 Loss: 0.07957691876575924\n",
      "Epoch 180/500, Training Loss: 0.07884801767658496, Smooth L1 Loss: 0.07957319066358301\n",
      "Epoch 181/500, Training Loss: 0.07880878583460614, Smooth L1 Loss: 0.07954201610902181\n",
      "Epoch 182/500, Training Loss: 0.07876649542131285, Smooth L1 Loss: 0.07954909242331408\n",
      "Epoch 183/500, Training Loss: 0.07871285987936932, Smooth L1 Loss: 0.0794935179157899\n",
      "Epoch 184/500, Training Loss: 0.07867894935363157, Smooth L1 Loss: 0.07949841629642133\n",
      "Epoch 185/500, Training Loss: 0.07862856980539175, Smooth L1 Loss: 0.07948616492705277\n",
      "Epoch 186/500, Training Loss: 0.07859425507665832, Smooth L1 Loss: 0.07944993158945671\n",
      "Epoch 187/500, Training Loss: 0.07854269171394587, Smooth L1 Loss: 0.0794453918038366\n",
      "Epoch 188/500, Training Loss: 0.0785047699863784, Smooth L1 Loss: 0.0794256256463436\n",
      "Epoch 189/500, Training Loss: 0.0784648650366327, Smooth L1 Loss: 0.07937529432372405\n",
      "Epoch 190/500, Training Loss: 0.0784187191458428, Smooth L1 Loss: 0.07934574537480679\n",
      "Epoch 191/500, Training Loss: 0.07836876865386387, Smooth L1 Loss: 0.07932491360518795\n",
      "Epoch 192/500, Training Loss: 0.07834720107668264, Smooth L1 Loss: 0.0792981585344443\n",
      "Epoch 193/500, Training Loss: 0.07829105018993507, Smooth L1 Loss: 0.07928811862634924\n",
      "Epoch 194/500, Training Loss: 0.07825984969591172, Smooth L1 Loss: 0.07927615294018044\n",
      "Epoch 195/500, Training Loss: 0.07820581689764912, Smooth L1 Loss: 0.07923194377396542\n",
      "Epoch 196/500, Training Loss: 0.07817189949722106, Smooth L1 Loss: 0.07920948245849174\n",
      "Epoch 197/500, Training Loss: 0.07812885641331833, Smooth L1 Loss: 0.0792111516571962\n",
      "Epoch 198/500, Training Loss: 0.07809645880535605, Smooth L1 Loss: 0.07921091545946322\n",
      "Epoch 199/500, Training Loss: 0.07805745785938945, Smooth L1 Loss: 0.07917824263971013\n",
      "Epoch 200/500, Training Loss: 0.07801686005963795, Smooth L1 Loss: 0.07916813521072842\n",
      "Epoch 201/500, Training Loss: 0.0779790844462344, Smooth L1 Loss: 0.07911812605407949\n",
      "Epoch 202/500, Training Loss: 0.07792898469069154, Smooth L1 Loss: 0.07909792568534613\n",
      "Epoch 203/500, Training Loss: 0.0779013279195569, Smooth L1 Loss: 0.07910925626324919\n",
      "Epoch 204/500, Training Loss: 0.07786314507973367, Smooth L1 Loss: 0.07906190973993105\n",
      "Epoch 205/500, Training Loss: 0.07783194285803947, Smooth L1 Loss: 0.07907442825559813\n",
      "Epoch 206/500, Training Loss: 0.07779102349554859, Smooth L1 Loss: 0.07900800237145561\n",
      "Epoch 207/500, Training Loss: 0.07775915453687382, Smooth L1 Loss: 0.07903782919479105\n",
      "Epoch 208/500, Training Loss: 0.07772004082439027, Smooth L1 Loss: 0.07900363462977111\n",
      "Epoch 209/500, Training Loss: 0.07768496314900508, Smooth L1 Loss: 0.07898340517511734\n",
      "Epoch 210/500, Training Loss: 0.0776452004801536, Smooth L1 Loss: 0.07894294343602198\n",
      "Epoch 211/500, Training Loss: 0.0776099378000135, Smooth L1 Loss: 0.07893505765913197\n",
      "Epoch 212/500, Training Loss: 0.07756922514614276, Smooth L1 Loss: 0.07891085143917455\n",
      "Epoch 213/500, Training Loss: 0.07753839997061784, Smooth L1 Loss: 0.07889855257235467\n",
      "Epoch 214/500, Training Loss: 0.07749728683904174, Smooth L1 Loss: 0.07886982650066224\n",
      "Epoch 215/500, Training Loss: 0.07746910017685614, Smooth L1 Loss: 0.0788738728644183\n",
      "Epoch 216/500, Training Loss: 0.07743325073649918, Smooth L1 Loss: 0.0788748033452206\n",
      "Epoch 217/500, Training Loss: 0.07741003512328373, Smooth L1 Loss: 0.07881419310489526\n",
      "Epoch 218/500, Training Loss: 0.0773613634676749, Smooth L1 Loss: 0.07881577226978081\n",
      "Epoch 219/500, Training Loss: 0.07732792355228162, Smooth L1 Loss: 0.07877447203589746\n",
      "Epoch 220/500, Training Loss: 0.07729522559953772, Smooth L1 Loss: 0.07877315632784022\n",
      "Epoch 221/500, Training Loss: 0.07725964398415769, Smooth L1 Loss: 0.07873724929343623\n",
      "Epoch 222/500, Training Loss: 0.07722400364596486, Smooth L1 Loss: 0.07873419652549693\n",
      "Epoch 223/500, Training Loss: 0.07718473060983391, Smooth L1 Loss: 0.07873748346733359\n",
      "Epoch 224/500, Training Loss: 0.07716239063780089, Smooth L1 Loss: 0.07870736239979473\n",
      "Epoch 225/500, Training Loss: 0.07713822997062679, Smooth L1 Loss: 0.0787034307140857\n",
      "Epoch 226/500, Training Loss: 0.07709567137242515, Smooth L1 Loss: 0.07866261378288843\n",
      "Epoch 227/500, Training Loss: 0.07707523743527522, Smooth L1 Loss: 0.0786284354981035\n",
      "Epoch 228/500, Training Loss: 0.07704525457128235, Smooth L1 Loss: 0.07866014665565811\n",
      "Epoch 229/500, Training Loss: 0.07700243527474611, Smooth L1 Loss: 0.07863729189221676\n",
      "Epoch 230/500, Training Loss: 0.07697000462507857, Smooth L1 Loss: 0.07859681272664322\n",
      "Epoch 231/500, Training Loss: 0.07693483714680165, Smooth L1 Loss: 0.07860783197415563\n",
      "Epoch 232/500, Training Loss: 0.07691404059673278, Smooth L1 Loss: 0.07856617165872684\n",
      "Epoch 233/500, Training Loss: 0.0768793998601068, Smooth L1 Loss: 0.07858010426235314\n",
      "Epoch 234/500, Training Loss: 0.07684943394888426, Smooth L1 Loss: 0.07856543990783393\n",
      "Epoch 235/500, Training Loss: 0.0768139424243411, Smooth L1 Loss: 0.07856672181962775\n",
      "Epoch 236/500, Training Loss: 0.07679365758878598, Smooth L1 Loss: 0.0785418702647663\n",
      "Epoch 237/500, Training Loss: 0.0767562398372065, Smooth L1 Loss: 0.07852233629315518\n",
      "Epoch 238/500, Training Loss: 0.07672957597291412, Smooth L1 Loss: 0.07850111849032916\n",
      "Epoch 239/500, Training Loss: 0.07669274411771608, Smooth L1 Loss: 0.07846500887535512\n",
      "Epoch 240/500, Training Loss: 0.07667800094842335, Smooth L1 Loss: 0.07846510041361818\n",
      "Epoch 241/500, Training Loss: 0.07664194404359025, Smooth L1 Loss: 0.07848818109442408\n",
      "Epoch 242/500, Training Loss: 0.07660887102429993, Smooth L1 Loss: 0.07845439880083387\n",
      "Epoch 243/500, Training Loss: 0.0765774791367388, Smooth L1 Loss: 0.07844288595235692\n",
      "Epoch 244/500, Training Loss: 0.07654890979545703, Smooth L1 Loss: 0.07845322399113613\n",
      "Epoch 245/500, Training Loss: 0.07652091567858982, Smooth L1 Loss: 0.07838690243303202\n",
      "Epoch 246/500, Training Loss: 0.07649257811515228, Smooth L1 Loss: 0.07843850169760677\n",
      "Epoch 247/500, Training Loss: 0.07646776787972681, Smooth L1 Loss: 0.07840164525147814\n",
      "Epoch 248/500, Training Loss: 0.07643309665690874, Smooth L1 Loss: 0.07837099594493899\n",
      "Epoch 249/500, Training Loss: 0.0764024870623137, Smooth L1 Loss: 0.07834126961489137\n",
      "Epoch 250/500, Training Loss: 0.07638274010828727, Smooth L1 Loss: 0.07829788090804449\n",
      "Epoch 251/500, Training Loss: 0.07635425734851095, Smooth L1 Loss: 0.0783620979816008\n",
      "Epoch 252/500, Training Loss: 0.07631835773371268, Smooth L1 Loss: 0.07832170259923889\n",
      "Epoch 253/500, Training Loss: 0.07629003847278834, Smooth L1 Loss: 0.07830276481735592\n",
      "Epoch 254/500, Training Loss: 0.07626801549236556, Smooth L1 Loss: 0.07829868351109326\n",
      "Epoch 255/500, Training Loss: 0.07624500745160569, Smooth L1 Loss: 0.0782849861201472\n",
      "Epoch 256/500, Training Loss: 0.07621616324868755, Smooth L1 Loss: 0.07828859067880191\n",
      "Epoch 257/500, Training Loss: 0.0761927963382956, Smooth L1 Loss: 0.07826689113146411\n",
      "Epoch 258/500, Training Loss: 0.07615930948784386, Smooth L1 Loss: 0.07825854304246604\n",
      "Epoch 259/500, Training Loss: 0.0761342071680631, Smooth L1 Loss: 0.07825122565890734\n",
      "Epoch 260/500, Training Loss: 0.07610411605469271, Smooth L1 Loss: 0.07819026613679643\n",
      "Epoch 261/500, Training Loss: 0.07608076726684823, Smooth L1 Loss: 0.0782310960885997\n",
      "Epoch 262/500, Training Loss: 0.07606591467840085, Smooth L1 Loss: 0.07819775035246633\n",
      "Epoch 263/500, Training Loss: 0.07603112370639607, Smooth L1 Loss: 0.07818145598643102\n",
      "Epoch 264/500, Training Loss: 0.07599925239017044, Smooth L1 Loss: 0.07814319890279037\n",
      "Epoch 265/500, Training Loss: 0.07597244732477815, Smooth L1 Loss: 0.07817824294145864\n",
      "Epoch 266/500, Training Loss: 0.07595781652607779, Smooth L1 Loss: 0.07817549425033996\n",
      "Epoch 267/500, Training Loss: 0.07591636385318738, Smooth L1 Loss: 0.07813968428840432\n",
      "Epoch 268/500, Training Loss: 0.07590400074847078, Smooth L1 Loss: 0.07812573741046855\n",
      "Epoch 269/500, Training Loss: 0.07587456094901919, Smooth L1 Loss: 0.07809626006592925\n",
      "Epoch 270/500, Training Loss: 0.07584751755503065, Smooth L1 Loss: 0.078121156951126\n",
      "Epoch 271/500, Training Loss: 0.07582939677120407, Smooth L1 Loss: 0.07809379390584162\n",
      "Epoch 272/500, Training Loss: 0.07579869210072186, Smooth L1 Loss: 0.07808285271032499\n",
      "Epoch 273/500, Training Loss: 0.07577519696475803, Smooth L1 Loss: 0.07808518370326895\n",
      "Epoch 274/500, Training Loss: 0.07575400085480893, Smooth L1 Loss: 0.07808788846103618\n",
      "Epoch 275/500, Training Loss: 0.07572329094732441, Smooth L1 Loss: 0.0780477211273347\n",
      "Epoch 276/500, Training Loss: 0.07570623419279061, Smooth L1 Loss: 0.07802758084681745\n",
      "Epoch 277/500, Training Loss: 0.07567558462781031, Smooth L1 Loss: 0.07800683291414036\n",
      "Epoch 278/500, Training Loss: 0.07564485699370288, Smooth L1 Loss: 0.0780324216466397\n",
      "Epoch 279/500, Training Loss: 0.0756311317824799, Smooth L1 Loss: 0.07804056867742194\n",
      "Epoch 280/500, Training Loss: 0.07559445667741956, Smooth L1 Loss: 0.07798250504912665\n",
      "Epoch 281/500, Training Loss: 0.0755758790506257, Smooth L1 Loss: 0.07800398068502545\n",
      "Epoch 282/500, Training Loss: 0.07555170371624582, Smooth L1 Loss: 0.07801990151906815\n",
      "Epoch 283/500, Training Loss: 0.0755350179610333, Smooth L1 Loss: 0.07798191516373593\n",
      "Epoch 284/500, Training Loss: 0.07550418940215295, Smooth L1 Loss: 0.07796827667894271\n",
      "Epoch 285/500, Training Loss: 0.07548648453709009, Smooth L1 Loss: 0.07795407087542117\n",
      "Epoch 286/500, Training Loss: 0.07545103046341219, Smooth L1 Loss: 0.07795753421333547\n",
      "Epoch 287/500, Training Loss: 0.07543758995795019, Smooth L1 Loss: 0.07794149126857519\n",
      "Epoch 288/500, Training Loss: 0.07540983982060266, Smooth L1 Loss: 0.07797033509884316\n",
      "Epoch 289/500, Training Loss: 0.0753896386511084, Smooth L1 Loss: 0.07797115677609466\n",
      "Epoch 290/500, Training Loss: 0.0753677999375811, Smooth L1 Loss: 0.07791586745028886\n",
      "Epoch 291/500, Training Loss: 0.07535095926788118, Smooth L1 Loss: 0.0779193125736828\n",
      "Epoch 292/500, Training Loss: 0.07531820485989253, Smooth L1 Loss: 0.07792668689328891\n",
      "Epoch 293/500, Training Loss: 0.07529348795468681, Smooth L1 Loss: 0.07787928608461069\n",
      "Epoch 294/500, Training Loss: 0.0752700901441816, Smooth L1 Loss: 0.077883218235981\n",
      "Epoch 295/500, Training Loss: 0.0752527088034844, Smooth L1 Loss: 0.07786626431446236\n",
      "Epoch 296/500, Training Loss: 0.07523418225096044, Smooth L1 Loss: 0.07788417318191093\n",
      "Epoch 297/500, Training Loss: 0.07520672675348135, Smooth L1 Loss: 0.07785769731092912\n",
      "Epoch 298/500, Training Loss: 0.07518878591737309, Smooth L1 Loss: 0.07784395915671037\n",
      "Epoch 299/500, Training Loss: 0.07516085800989239, Smooth L1 Loss: 0.07783932471647859\n",
      "Epoch 300/500, Training Loss: 0.07514451555727761, Smooth L1 Loss: 0.07780167865208708\n",
      "Epoch 301/500, Training Loss: 0.07512207950154941, Smooth L1 Loss: 0.07781453468263723\n",
      "Epoch 302/500, Training Loss: 0.07508993103812282, Smooth L1 Loss: 0.07779765157745434\n",
      "Epoch 303/500, Training Loss: 0.07507632377642941, Smooth L1 Loss: 0.07780438319493371\n",
      "Epoch 304/500, Training Loss: 0.0750584702776826, Smooth L1 Loss: 0.07781763351522386\n",
      "Epoch 305/500, Training Loss: 0.07503139936693624, Smooth L1 Loss: 0.07780736149288714\n",
      "Epoch 306/500, Training Loss: 0.0750263725105979, Smooth L1 Loss: 0.07777499660061529\n",
      "Epoch 307/500, Training Loss: 0.0749908533613175, Smooth L1 Loss: 0.07778351066204217\n",
      "Epoch 308/500, Training Loss: 0.07496435938465998, Smooth L1 Loss: 0.07773521915078163\n",
      "Epoch 309/500, Training Loss: 0.07494412528143989, Smooth L1 Loss: 0.0777498628371037\n",
      "Epoch 310/500, Training Loss: 0.07491972723948782, Smooth L1 Loss: 0.07771215929936331\n",
      "Epoch 311/500, Training Loss: 0.074904623310923, Smooth L1 Loss: 0.07772861963782746\n",
      "Epoch 312/500, Training Loss: 0.07488297372336548, Smooth L1 Loss: 0.0777196882543369\n",
      "Epoch 313/500, Training Loss: 0.07486392984139746, Smooth L1 Loss: 0.07771798053111595\n",
      "Epoch 314/500, Training Loss: 0.07484354190348427, Smooth L1 Loss: 0.0776969833084597\n",
      "Epoch 315/500, Training Loss: 0.07482538199511127, Smooth L1 Loss: 0.0777040257727584\n",
      "Epoch 316/500, Training Loss: 0.0748010998510796, Smooth L1 Loss: 0.07768227249527207\n",
      "Epoch 317/500, Training Loss: 0.07478009445079858, Smooth L1 Loss: 0.07770528028217646\n",
      "Epoch 318/500, Training Loss: 0.07475886333751794, Smooth L1 Loss: 0.07771526551089035\n",
      "Epoch 319/500, Training Loss: 0.07472897869881225, Smooth L1 Loss: 0.07767352868373004\n",
      "Epoch 320/500, Training Loss: 0.07471921532482341, Smooth L1 Loss: 0.07765162342156355\n",
      "Epoch 321/500, Training Loss: 0.07469465428792336, Smooth L1 Loss: 0.07765734070338882\n",
      "Epoch 322/500, Training Loss: 0.07467554943357113, Smooth L1 Loss: 0.07762818348307449\n",
      "Epoch 323/500, Training Loss: 0.07464938542405189, Smooth L1 Loss: 0.07766029519888644\n",
      "Epoch 324/500, Training Loss: 0.07463252402230161, Smooth L1 Loss: 0.07761430165443856\n",
      "Epoch 325/500, Training Loss: 0.07461715968334733, Smooth L1 Loss: 0.0776045743125276\n",
      "Epoch 326/500, Training Loss: 0.07459623969479459, Smooth L1 Loss: 0.07761847568102755\n",
      "Epoch 327/500, Training Loss: 0.07457400953784081, Smooth L1 Loss: 0.07757212981008567\n",
      "Epoch 328/500, Training Loss: 0.07456107047085025, Smooth L1 Loss: 0.07760442338454035\n",
      "Epoch 329/500, Training Loss: 0.0745335799700396, Smooth L1 Loss: 0.07757510470512968\n",
      "Epoch 330/500, Training Loss: 0.07452119584532751, Smooth L1 Loss: 0.07760695736998549\n",
      "Epoch 331/500, Training Loss: 0.07450214777015833, Smooth L1 Loss: 0.0775849932912164\n",
      "Epoch 332/500, Training Loss: 0.0744734668681299, Smooth L1 Loss: 0.07756999045467147\n",
      "Epoch 333/500, Training Loss: 0.0744569122719304, Smooth L1 Loss: 0.07757396435436721\n",
      "Epoch 334/500, Training Loss: 0.07444016513041252, Smooth L1 Loss: 0.0775381638847578\n",
      "Epoch 335/500, Training Loss: 0.07441508114913811, Smooth L1 Loss: 0.07751662679947913\n",
      "Epoch 336/500, Training Loss: 0.07439493792860405, Smooth L1 Loss: 0.07751866698694918\n",
      "Epoch 337/500, Training Loss: 0.07438610760486068, Smooth L1 Loss: 0.07752520908028461\n",
      "Epoch 338/500, Training Loss: 0.07435689155678242, Smooth L1 Loss: 0.07754625270787913\n",
      "Epoch 339/500, Training Loss: 0.07433351980531273, Smooth L1 Loss: 0.07753298220296319\n",
      "Epoch 340/500, Training Loss: 0.07432212417828288, Smooth L1 Loss: 0.0774943578379372\n",
      "Epoch 341/500, Training Loss: 0.07430512832414701, Smooth L1 Loss: 0.07752560331629446\n",
      "Epoch 342/500, Training Loss: 0.07428066983171132, Smooth L1 Loss: 0.0774646804560549\n",
      "Epoch 343/500, Training Loss: 0.0742657030010281, Smooth L1 Loss: 0.07748933829582082\n",
      "Epoch 344/500, Training Loss: 0.07424522371251802, Smooth L1 Loss: 0.07750829062066399\n",
      "Epoch 345/500, Training Loss: 0.07422407187413478, Smooth L1 Loss: 0.07745456188702239\n",
      "Epoch 346/500, Training Loss: 0.07420478354473621, Smooth L1 Loss: 0.07747180842292997\n",
      "Epoch 347/500, Training Loss: 0.07418164558672674, Smooth L1 Loss: 0.07747924811421679\n",
      "Epoch 348/500, Training Loss: 0.0741649866104126, Smooth L1 Loss: 0.07743523135566367\n",
      "Epoch 349/500, Training Loss: 0.07415711705163482, Smooth L1 Loss: 0.077421794914139\n",
      "Epoch 350/500, Training Loss: 0.07412221489681138, Smooth L1 Loss: 0.07744569094994894\n",
      "Epoch 351/500, Training Loss: 0.07412103052444505, Smooth L1 Loss: 0.0774403488657509\n",
      "Epoch 352/500, Training Loss: 0.07409501365489429, Smooth L1 Loss: 0.07743912611085062\n",
      "Epoch 353/500, Training Loss: 0.07407665470444062, Smooth L1 Loss: 0.07742514937686232\n",
      "Epoch 354/500, Training Loss: 0.07405478129352348, Smooth L1 Loss: 0.07740471406409946\n",
      "Epoch 355/500, Training Loss: 0.07404614106756477, Smooth L1 Loss: 0.07742581380387911\n",
      "Epoch 356/500, Training Loss: 0.07401558198934592, Smooth L1 Loss: 0.07737746704011582\n",
      "Epoch 357/500, Training Loss: 0.07400374562628027, Smooth L1 Loss: 0.07738723799299735\n",
      "Epoch 358/500, Training Loss: 0.0739800449874666, Smooth L1 Loss: 0.0773800381172735\n",
      "Epoch 359/500, Training Loss: 0.07396507365764052, Smooth L1 Loss: 0.07737603563313875\n",
      "Epoch 360/500, Training Loss: 0.0739444375290099, Smooth L1 Loss: 0.07733686601456541\n",
      "Epoch 361/500, Training Loss: 0.07392796632892268, Smooth L1 Loss: 0.07736353337979661\n",
      "Epoch 362/500, Training Loss: 0.07392148569154279, Smooth L1 Loss: 0.07734048810715859\n",
      "Epoch 363/500, Training Loss: 0.07389958271225869, Smooth L1 Loss: 0.07733512392195944\n",
      "Epoch 364/500, Training Loss: 0.07387373531642168, Smooth L1 Loss: 0.07733285310678184\n",
      "Epoch 365/500, Training Loss: 0.07385854090109539, Smooth L1 Loss: 0.07730582089593205\n",
      "Epoch 366/500, Training Loss: 0.07383312502704957, Smooth L1 Loss: 0.07733282283879817\n",
      "Epoch 367/500, Training Loss: 0.07382086425083847, Smooth L1 Loss: 0.07734126995245998\n",
      "Epoch 368/500, Training Loss: 0.07380246757957094, Smooth L1 Loss: 0.07731917183488034\n",
      "Epoch 369/500, Training Loss: 0.07378258343768004, Smooth L1 Loss: 0.07732421922712372\n",
      "Epoch 370/500, Training Loss: 0.0737729336940436, Smooth L1 Loss: 0.07733779131936339\n",
      "Epoch 371/500, Training Loss: 0.07375392483340369, Smooth L1 Loss: 0.07725506587527119\n",
      "Epoch 372/500, Training Loss: 0.0737360545978454, Smooth L1 Loss: 0.07728251100231248\n",
      "Epoch 373/500, Training Loss: 0.07370638638590844, Smooth L1 Loss: 0.07728071009310392\n",
      "Epoch 374/500, Training Loss: 0.0736996077792944, Smooth L1 Loss: 0.07729655228411922\n",
      "Epoch 375/500, Training Loss: 0.0736827217762309, Smooth L1 Loss: 0.07725868112622546\n",
      "Epoch 376/500, Training Loss: 0.07366459249802258, Smooth L1 Loss: 0.07724384127113108\n",
      "Epoch 377/500, Training Loss: 0.07365057612026947, Smooth L1 Loss: 0.07722965753279053\n",
      "Epoch 378/500, Training Loss: 0.07362981720103158, Smooth L1 Loss: 0.0772525544027583\n",
      "Epoch 379/500, Training Loss: 0.07360832672116262, Smooth L1 Loss: 0.07728044717357709\n",
      "Epoch 380/500, Training Loss: 0.07359624994189842, Smooth L1 Loss: 0.07725820428906725\n",
      "Epoch 381/500, Training Loss: 0.07357934387265772, Smooth L1 Loss: 0.07720375861614369\n",
      "Epoch 382/500, Training Loss: 0.07356216846672808, Smooth L1 Loss: 0.07723950231089614\n",
      "Epoch 383/500, Training Loss: 0.07355009085962162, Smooth L1 Loss: 0.07719612560378245\n",
      "Epoch 384/500, Training Loss: 0.07352284644392953, Smooth L1 Loss: 0.07724855086193062\n",
      "Epoch 385/500, Training Loss: 0.07351465982155524, Smooth L1 Loss: 0.07717370594708392\n",
      "Epoch 386/500, Training Loss: 0.07349571895196243, Smooth L1 Loss: 0.07716263141124867\n",
      "Epoch 387/500, Training Loss: 0.07347580261420512, Smooth L1 Loss: 0.07719162532773155\n",
      "Epoch 388/500, Training Loss: 0.07345270448260838, Smooth L1 Loss: 0.07717442537586276\n",
      "Epoch 389/500, Training Loss: 0.07345173326163476, Smooth L1 Loss: 0.07718693740809193\n",
      "Epoch 390/500, Training Loss: 0.07342243685886479, Smooth L1 Loss: 0.0771777277919822\n",
      "Epoch 391/500, Training Loss: 0.07341909584935737, Smooth L1 Loss: 0.07719786483078049\n",
      "Epoch 392/500, Training Loss: 0.07339732014182684, Smooth L1 Loss: 0.07711694276748368\n",
      "Epoch 393/500, Training Loss: 0.07337730039576977, Smooth L1 Loss: 0.07714213171185783\n",
      "Epoch 394/500, Training Loss: 0.07336630251096643, Smooth L1 Loss: 0.07718382136394772\n",
      "Epoch 395/500, Training Loss: 0.07334302497154849, Smooth L1 Loss: 0.0771659082876375\n",
      "Epoch 396/500, Training Loss: 0.07332541404858879, Smooth L1 Loss: 0.07714251407350485\n",
      "Epoch 397/500, Training Loss: 0.0733159891181234, Smooth L1 Loss: 0.07712740725121246\n",
      "Epoch 398/500, Training Loss: 0.0732921063180131, Smooth L1 Loss: 0.07714918931014836\n",
      "Epoch 399/500, Training Loss: 0.0732855270234283, Smooth L1 Loss: 0.0771127245747126\n",
      "Epoch 400/500, Training Loss: 0.07326670995225078, Smooth L1 Loss: 0.07711170486604366\n",
      "Epoch 401/500, Training Loss: 0.07324041544959164, Smooth L1 Loss: 0.07711876190912265\n",
      "Epoch 402/500, Training Loss: 0.07323859165904027, Smooth L1 Loss: 0.07711804962645356\n",
      "Epoch 403/500, Training Loss: 0.07321624150094778, Smooth L1 Loss: 0.07709379558666395\n",
      "Epoch 404/500, Training Loss: 0.07320348963429386, Smooth L1 Loss: 0.07711748207489458\n",
      "Epoch 405/500, Training Loss: 0.07318651222664377, Smooth L1 Loss: 0.07711057655083445\n",
      "Epoch 406/500, Training Loss: 0.07317270662473596, Smooth L1 Loss: 0.07712002700337997\n",
      "Epoch 407/500, Training Loss: 0.0731508269150188, Smooth L1 Loss: 0.07705014200809483\n",
      "Epoch 408/500, Training Loss: 0.07313528633563991, Smooth L1 Loss: 0.07708279928192496\n",
      "Epoch 409/500, Training Loss: 0.07311997929776924, Smooth L1 Loss: 0.0770882734706482\n",
      "Epoch 410/500, Training Loss: 0.07310052703760096, Smooth L1 Loss: 0.07703628625649099\n",
      "Epoch 411/500, Training Loss: 0.0730882907367271, Smooth L1 Loss: 0.07706689985039142\n",
      "Epoch 412/500, Training Loss: 0.07307200992236966, Smooth L1 Loss: 0.07706702751322435\n",
      "Epoch 413/500, Training Loss: 0.0730493993718843, Smooth L1 Loss: 0.07707576480550835\n",
      "Epoch 414/500, Training Loss: 0.07304308745236213, Smooth L1 Loss: 0.07705229416919443\n",
      "Epoch 415/500, Training Loss: 0.07302856889827816, Smooth L1 Loss: 0.07702568682053915\n",
      "Epoch 416/500, Training Loss: 0.0730197190662513, Smooth L1 Loss: 0.07706660886581701\n",
      "Epoch 417/500, Training Loss: 0.07300396654124996, Smooth L1 Loss: 0.07707650690840986\n",
      "Epoch 418/500, Training Loss: 0.07297554723738472, Smooth L1 Loss: 0.07702886026639205\n",
      "Epoch 419/500, Training Loss: 0.0729591564639755, Smooth L1 Loss: 0.07698135819429389\n",
      "Epoch 420/500, Training Loss: 0.07295341827515242, Smooth L1 Loss: 0.07700572547932658\n",
      "Epoch 421/500, Training Loss: 0.07292732733171343, Smooth L1 Loss: 0.07697700508511983\n",
      "Epoch 422/500, Training Loss: 0.07291025061898185, Smooth L1 Loss: 0.07698504060793382\n",
      "Epoch 423/500, Training Loss: 0.07289999550667361, Smooth L1 Loss: 0.0770018116141168\n",
      "Epoch 424/500, Training Loss: 0.0728872173110356, Smooth L1 Loss: 0.07696166494861245\n",
      "Epoch 425/500, Training Loss: 0.07287103797934481, Smooth L1 Loss: 0.07700267478895302\n",
      "Epoch 426/500, Training Loss: 0.07285708039639077, Smooth L1 Loss: 0.0769827169760202\n",
      "Epoch 427/500, Training Loss: 0.07282530161853573, Smooth L1 Loss: 0.07696147875573772\n",
      "Epoch 428/500, Training Loss: 0.07281847022797751, Smooth L1 Loss: 0.07696773659867737\n",
      "Epoch 429/500, Training Loss: 0.07280134193707204, Smooth L1 Loss: 0.07694139080838515\n",
      "Epoch 430/500, Training Loss: 0.07279271211313165, Smooth L1 Loss: 0.07698358787008776\n",
      "Epoch 431/500, Training Loss: 0.07277174590938333, Smooth L1 Loss: 0.07694225942787643\n",
      "Epoch 432/500, Training Loss: 0.07276848851194705, Smooth L1 Loss: 0.07697743682477337\n",
      "Epoch 433/500, Training Loss: 0.07274522376809144, Smooth L1 Loss: 0.07695182520323075\n",
      "Epoch 434/500, Training Loss: 0.07273335443530682, Smooth L1 Loss: 0.07691585840299152\n",
      "Epoch 435/500, Training Loss: 0.07272085381878747, Smooth L1 Loss: 0.07694627335653283\n",
      "Epoch 436/500, Training Loss: 0.07270453170229847, Smooth L1 Loss: 0.07691291839672396\n",
      "Epoch 437/500, Training Loss: 0.07268739408916897, Smooth L1 Loss: 0.0769124747468875\n",
      "Epoch 438/500, Training Loss: 0.07266472096460452, Smooth L1 Loss: 0.07693437889863092\n",
      "Epoch 439/500, Training Loss: 0.07265250383007929, Smooth L1 Loss: 0.07692123408644246\n",
      "Epoch 440/500, Training Loss: 0.07264123677055617, Smooth L1 Loss: 0.07694985164114489\n",
      "Epoch 441/500, Training Loss: 0.07262773036164938, Smooth L1 Loss: 0.0768854904275101\n",
      "Epoch 442/500, Training Loss: 0.0726093088551132, Smooth L1 Loss: 0.0768799116082776\n",
      "Epoch 443/500, Training Loss: 0.07259872190402326, Smooth L1 Loss: 0.07688054154053904\n",
      "Epoch 444/500, Training Loss: 0.07259152410326948, Smooth L1 Loss: 0.076891056011216\n",
      "Epoch 445/500, Training Loss: 0.07256209150244648, Smooth L1 Loss: 0.07689962557588632\n",
      "Epoch 446/500, Training Loss: 0.07254893078968144, Smooth L1 Loss: 0.07682680576824798\n",
      "Epoch 447/500, Training Loss: 0.0725455529951819, Smooth L1 Loss: 0.07688162801787257\n",
      "Epoch 448/500, Training Loss: 0.07251181969968017, Smooth L1 Loss: 0.07686429091084462\n",
      "Epoch 449/500, Training Loss: 0.07250326898866806, Smooth L1 Loss: 0.07688897466645218\n",
      "Epoch 450/500, Training Loss: 0.07249965749069112, Smooth L1 Loss: 0.07686857843341735\n",
      "Epoch 451/500, Training Loss: 0.07247765583620555, Smooth L1 Loss: 0.07688712984180221\n",
      "Epoch 452/500, Training Loss: 0.07245752346760409, Smooth L1 Loss: 0.07687488447230023\n",
      "Epoch 453/500, Training Loss: 0.07244311766204051, Smooth L1 Loss: 0.07681559739061274\n",
      "Epoch 454/500, Training Loss: 0.0724403346542287, Smooth L1 Loss: 0.07683732765368544\n",
      "Epoch 455/500, Training Loss: 0.07242714219551155, Smooth L1 Loss: 0.07682912160929006\n",
      "Epoch 456/500, Training Loss: 0.07241236094547354, Smooth L1 Loss: 0.07680265485452345\n",
      "Epoch 457/500, Training Loss: 0.07238947814285467, Smooth L1 Loss: 0.07683475123933302\n",
      "Epoch 458/500, Training Loss: 0.07238260402843572, Smooth L1 Loss: 0.07677573502923433\n",
      "Epoch 459/500, Training Loss: 0.07236361365027474, Smooth L1 Loss: 0.07683499807563539\n",
      "Epoch 460/500, Training Loss: 0.07234921774282548, Smooth L1 Loss: 0.07683720801455471\n",
      "Epoch 461/500, Training Loss: 0.07233516900723683, Smooth L1 Loss: 0.0768068314780696\n",
      "Epoch 462/500, Training Loss: 0.07231765681346833, Smooth L1 Loss: 0.07680680443389484\n",
      "Epoch 463/500, Training Loss: 0.07231165990593352, Smooth L1 Loss: 0.07681967588499762\n",
      "Epoch 464/500, Training Loss: 0.0722936577015165, Smooth L1 Loss: 0.07679972595248657\n",
      "Epoch 465/500, Training Loss: 0.07228614447485422, Smooth L1 Loss: 0.07677731125687177\n",
      "Epoch 466/500, Training Loss: 0.07226165176193783, Smooth L1 Loss: 0.07679908778160237\n",
      "Epoch 467/500, Training Loss: 0.07225277465610688, Smooth L1 Loss: 0.07677104947372125\n",
      "Epoch 468/500, Training Loss: 0.07223770864631819, Smooth L1 Loss: 0.07677645032079174\n",
      "Epoch 469/500, Training Loss: 0.07222339653090579, Smooth L1 Loss: 0.07676985537490019\n",
      "Epoch 470/500, Training Loss: 0.07221041815940309, Smooth L1 Loss: 0.07678353800796546\n",
      "Epoch 471/500, Training Loss: 0.07218477993771649, Smooth L1 Loss: 0.07678571929080555\n",
      "Epoch 472/500, Training Loss: 0.07217786698669627, Smooth L1 Loss: 0.07674241264780554\n",
      "Epoch 473/500, Training Loss: 0.07217202090842713, Smooth L1 Loss: 0.07676337098774429\n",
      "Epoch 474/500, Training Loss: 0.07215397750985796, Smooth L1 Loss: 0.07672424331450692\n",
      "Epoch 475/500, Training Loss: 0.0721392998120923, Smooth L1 Loss: 0.07670674605581623\n",
      "Epoch 476/500, Training Loss: 0.07212257901755507, Smooth L1 Loss: 0.0767525945467731\n",
      "Epoch 477/500, Training Loss: 0.07211750582004516, Smooth L1 Loss: 0.07671860053848761\n",
      "Epoch 478/500, Training Loss: 0.07208874409541416, Smooth L1 Loss: 0.076729706954211\n",
      "Epoch 479/500, Training Loss: 0.07208510923788743, Smooth L1 Loss: 0.07672550310738958\n",
      "Epoch 480/500, Training Loss: 0.07206752669552098, Smooth L1 Loss: 0.07672301038669851\n",
      "Epoch 481/500, Training Loss: 0.07204916142827071, Smooth L1 Loss: 0.07670114346994804\n",
      "Epoch 482/500, Training Loss: 0.0720407055231972, Smooth L1 Loss: 0.07672778850134748\n",
      "Epoch 483/500, Training Loss: 0.07202719207763096, Smooth L1 Loss: 0.07670207010008968\n",
      "Epoch 484/500, Training Loss: 0.07201664162358799, Smooth L1 Loss: 0.07670479027840954\n",
      "Epoch 485/500, Training Loss: 0.07199643986020687, Smooth L1 Loss: 0.07669937132427898\n",
      "Epoch 486/500, Training Loss: 0.07197979897044707, Smooth L1 Loss: 0.07671069235612567\n",
      "Epoch 487/500, Training Loss: 0.07197767204564551, Smooth L1 Loss: 0.07671809311096485\n",
      "Epoch 488/500, Training Loss: 0.071957610025642, Smooth L1 Loss: 0.07668836579586451\n",
      "Epoch 489/500, Training Loss: 0.07193937678124018, Smooth L1 Loss: 0.07669540568111607\n",
      "Epoch 490/500, Training Loss: 0.07193980499166222, Smooth L1 Loss: 0.07670288959231514\n",
      "Epoch 491/500, Training Loss: 0.07192046918730804, Smooth L1 Loss: 0.07669276903526714\n",
      "Epoch 492/500, Training Loss: 0.07191327471592, Smooth L1 Loss: 0.07665456389077008\n",
      "Epoch 493/500, Training Loss: 0.07189410052077781, Smooth L1 Loss: 0.07668692367867781\n",
      "Epoch 494/500, Training Loss: 0.07188381058510375, Smooth L1 Loss: 0.07666541806135613\n",
      "Epoch 495/500, Training Loss: 0.07185093495220954, Smooth L1 Loss: 0.0766736229057782\n",
      "Epoch 496/500, Training Loss: 0.07185382424777256, Smooth L1 Loss: 0.07665514304804114\n",
      "Epoch 497/500, Training Loss: 0.07183735855463622, Smooth L1 Loss: 0.0766556656632859\n",
      "Epoch 498/500, Training Loss: 0.0718307513113759, Smooth L1 Loss: 0.07662033733840172\n",
      "Epoch 499/500, Training Loss: 0.07181200300509803, Smooth L1 Loss: 0.07664244272746146\n",
      "Epoch 500/500, Training Loss: 0.07179183257374787, Smooth L1 Loss: 0.07663992341034688\n"
     ]
    }
   ],
   "source": [
    "# Storing data for visually observing the losses\n",
    "losses = {\"training\" : [], \n",
    "          \"evaluation\" : [], \n",
    "          \"labels\" : [], \n",
    "          \"outputs\" : [],\n",
    "          \"labels_t\" : [], \n",
    "          \"outputs_t\" : []\n",
    "          }\n",
    "\n",
    "# Training & Evaluation Porcess\n",
    "patience = 15                      # Patience based stopping criteria. You may also play around with this number for early or delayed stopping of the model from over-fitting\n",
    "best_loss = float('inf')\n",
    "num_epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    all_outputs_training = []\n",
    "    all_labels_training = []\n",
    "    model.train(True)\n",
    "    running_loss = 0.0                          # To calculates the losses in training for each epoch\n",
    "    for inputs, labels in train_loader:\n",
    "        # Transfers data to GPU/CPU\n",
    "        inputs = inputs.to(device)              \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()                   # Zeros the optimizer before generating output\n",
    "        outputs = model(inputs)                 # Calculates the output\n",
    "        loss = criterion(outputs, labels)       # Finds loss as per the criteria defined\n",
    "        loss.backward()                         # Back propogation of loss\n",
    "        optimizer.step()                        # Updates parameters based on gradients computed duing back propogation\n",
    "        \n",
    "        running_loss += loss.item()             # Calculates loss over the training\n",
    "\n",
    "        all_outputs_training.append(outputs.detach().numpy())  # Append outputs to the list\n",
    "        all_labels_training.append(labels.detach().numpy())    # Append labels to the list\n",
    "    \n",
    "    training_loss = running_loss/len(train_loader)\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluating the model that has been trained (so far). \n",
    "    You would notice similar steps as during the training process. The lack of a few lines of code is because we \n",
    "    are evaluating the model here and not training it.\n",
    "    \"\"\"\n",
    "    model.train(False)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_outputs.append(outputs.cpu().numpy())  \n",
    "            all_labels.append(labels.cpu().numpy())    \n",
    "                \n",
    "    mean_loss = total_loss / len(test_loader)\n",
    "\n",
    "    if mean_loss < best_loss:\n",
    "        best_loss = mean_loss\n",
    "        num_epochs_without_improvement = 0\n",
    "    else:\n",
    "        num_epochs_without_improvement += 1\n",
    "        if num_epochs_without_improvement == patience:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "    \n",
    "    # Recording the data values for\n",
    "    losses[\"training\"].append(training_loss)\n",
    "    losses[\"evaluation\"].append(mean_loss)\n",
    "    losses[\"outputs\"] = np.concatenate(all_outputs)\n",
    "    losses[\"labels\"] = np.concatenate(all_labels)\n",
    "    \n",
    "    losses[\"outputs_t\"] = np.concatenate(all_outputs_training)\n",
    "    losses[\"labels_t\"] = np.concatenate(all_labels_training)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {training_loss}, Smooth L1 Loss: {mean_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Performance Analysis & Visualization\n",
    "\n",
    "The model's training and evaluation losses over an epochs curve help to analyze its learning progress and detect signs of overfitting or underfitting. Analyzing these curves aids in fine-tuning the model's architecture or regularization techniques to achieve a balance between accuracy and generalization.\n",
    "\n",
    "Since we will be using the neural networks to estimate the X, Y & Z coordinates, a 3D visualization represents the outputs and actual labels to gain a deeper understanding of the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lossPlot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.8,
          "size": 3
         },
         "mode": "markers",
         "name": "Input Points",
         "type": "scatter3d",
         "x": [
          -1.4085233211517334,
          -1.4085233211517334,
          -1.4092185497283936,
          -1.4101617336273193,
          -1.4150595664978027,
          -1.4161475896835327,
          -1.4169338941574097,
          -1.422666072845459,
          -1.424009919166565,
          -1.424009919166565,
          -1.4250379800796509,
          -1.426499366760254,
          -1.4297820329666138,
          -1.4297820329666138,
          -1.431680679321289,
          -1.433286190032959,
          -1.4413433074951172,
          -1.4413433074951172,
          -1.4480559825897217,
          -1.457503080368042,
          -1.4518353939056396,
          -1.4496657848358154,
          -1.446548581123352,
          -1.4418609142303467,
          -1.4365302324295044,
          -1.4329943656921387,
          -1.4277931451797485,
          -1.4209909439086914,
          -1.4102574586868286,
          -1.4048913717269897,
          -1.3975802659988403,
          -1.3954616785049438,
          -1.3577388525009155,
          -1.3492873907089233,
          -1.3417757749557495,
          -1.3103657960891724,
          -1.2911783456802368,
          -1.2911783456802368,
          -1.2921831607818604,
          -1.282472848892212
         ],
         "y": [
          0.42588138580322266,
          0.42588138580322266,
          0.4239582419395447,
          0.42174863815307617,
          0.4128790497779846,
          0.40984606742858887,
          0.40706366300582886,
          0.39769527316093445,
          0.395425021648407,
          0.395425021648407,
          0.3926893174648285,
          0.3905506730079651,
          0.3859063386917114,
          0.3859063386917114,
          0.38388678431510925,
          0.3819429874420166,
          0.3716640770435333,
          0.3716640770435333,
          0.3677234947681427,
          0.36866670846939087,
          0.37558966875076294,
          0.3783063292503357,
          0.3811613917350769,
          0.3912031352519989,
          0.3920094072818756,
          0.3963356018066406,
          0.39806345105171204,
          0.3994615972042084,
          0.40733471512794495,
          0.4119243323802948,
          0.41468334197998047,
          0.4251401424407959,
          0.4281245470046997,
          0.4313385784626007,
          0.4355641305446625,
          0.42961522936820984,
          0.43879732489585876,
          0.43879732489585876,
          0.45113256573677063,
          0.45463472604751587
         ],
         "z": [
          -2.048785924911499,
          -2.048785924911499,
          -2.0490939617156982,
          -2.0490691661834717,
          -2.0490472316741943,
          -2.0493741035461426,
          -2.049234390258789,
          -2.0520095825195312,
          -2.0529682636260986,
          -2.0529682636260986,
          -2.0538668632507324,
          -2.0542426109313965,
          -2.054429292678833,
          -2.054429292678833,
          -2.0540459156036377,
          -2.0541205406188965,
          -2.0468204021453857,
          -2.0468204021453857,
          -2.037379503250122,
          -1.8905534744262695,
          -1.7878094911575317,
          -1.7594738006591797,
          -1.7296444177627563,
          -1.668265461921692,
          -1.63782799243927,
          -1.6035104990005493,
          -1.5725818872451782,
          -1.5391839742660522,
          -1.4685513973236084,
          -1.433737874031067,
          -1.3978713750839233,
          -1.3528878688812256,
          -1.2158735990524292,
          -1.1796460151672363,
          -1.1449614763259888,
          -1.074773907661438,
          -1.0128978490829468,
          -1.0128978490829468,
          -0.9747163653373718,
          -0.9425485134124756
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 3
         },
         "mode": "markers",
         "name": "Output Points",
         "type": "scatter3d",
         "x": [
          -0.9095962643623352,
          -0.8977863788604736,
          -0.9215601086616516,
          -0.8955082297325134,
          -0.9116998314857483,
          -0.9167875051498413,
          -0.9316497445106506,
          -0.9327139258384705,
          -0.9678126573562622,
          -0.9848368167877197,
          -1.0251593589782715,
          -1.058150053024292,
          -1.104621171951294,
          -1.092577576637268,
          -1.1671257019042969,
          -1.1621760129928589,
          -1.1514440774917603,
          -1.1514198780059814,
          -1.061743974685669,
          -0.9959738254547119,
          -0.8928559422492981,
          -0.9529713988304138,
          -0.9157680869102478,
          -0.9240633845329285,
          -0.8820675015449524,
          -0.8321992754936218,
          -0.8290689587593079,
          -0.7806012034416199,
          -0.8158221244812012,
          -0.8749206066131592,
          -0.9016255140304565,
          -0.898113489151001,
          -0.8843944072723389,
          -0.8868147730827332,
          -0.8994163870811462,
          -0.8961861729621887,
          -0.9169260859489441,
          -0.9226610064506531,
          -0.9183303117752075,
          -0.9184659123420715
         ],
         "y": [
          1.9708749055862427,
          1.9266645908355713,
          1.9228090047836304,
          1.9459112882614136,
          1.9641135931015015,
          2.0394978523254395,
          2.073596954345703,
          2.0986485481262207,
          2.1569411754608154,
          2.191859006881714,
          2.216916561126709,
          2.307286262512207,
          2.361001491546631,
          2.259951114654541,
          2.1687307357788086,
          2.0510358810424805,
          2.065870761871338,
          2.0423262119293213,
          2.112968921661377,
          2.2780134677886963,
          2.451781749725342,
          2.6468751430511475,
          2.7151994705200195,
          2.77539324760437,
          2.774068832397461,
          2.5209295749664307,
          2.2799360752105713,
          2.2614638805389404,
          2.296293258666992,
          2.326815366744995,
          2.3457016944885254,
          2.357011079788208,
          2.406297445297241,
          2.375270128250122,
          2.4044430255889893,
          2.3983006477355957,
          2.4051361083984375,
          2.392019033432007,
          2.388216495513916,
          2.4140164852142334
         ],
         "z": [
          -1.1758326292037964,
          -1.2240538597106934,
          -1.2534223794937134,
          -1.2056725025177002,
          -1.2051573991775513,
          -1.1242644786834717,
          -1.074111819267273,
          -1.0720634460449219,
          -0.993390679359436,
          -0.9946157932281494,
          -1.0168185234069824,
          -1.0304372310638428,
          -1.115649938583374,
          -1.209351658821106,
          -1.2385212182998657,
          -1.1338132619857788,
          -1.0440069437026978,
          -0.961622953414917,
          -0.9499057531356812,
          -0.9868264198303223,
          -1.03230619430542,
          -0.9326716661453247,
          -0.7824393510818481,
          -0.7924332022666931,
          -0.804162859916687,
          -0.8294453620910645,
          -0.7645791172981262,
          -0.7209001183509827,
          -0.7444941997528076,
          -0.7611599564552307,
          -0.7628778219223022,
          -0.7762289643287659,
          -0.7922424674034119,
          -0.8590320944786072,
          -0.8694506883621216,
          -0.8634498715400696,
          -0.8903594017028809,
          -0.8510280847549438,
          -0.8400954604148865,
          -0.830237627029419
         ]
        }
       ],
       "layout": {
        "scene": {
         "xaxis": {
          "title": {
           "text": "X-Dimension"
          }
         },
         "yaxis": {
          "title": {
           "text": "Y-Dimension"
          }
         },
         "zaxis": {
          "title": {
           "text": "Z-Dimension"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Input and Output Points in 3D Space"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.8,
          "size": 3
         },
         "mode": "markers",
         "name": "Input Points",
         "type": "scatter3d",
         "x": [
          -0.6812216639518738,
          0.1889086812734604,
          -1.3466966152191162,
          0.2783301770687103,
          1.2118092775344849,
          1.1359913349151611,
          0.1290571242570877,
          -0.45975714921951294,
          -0.7662709355354309,
          0.08589743822813034,
          -1.1698801517486572,
          1.9257700443267822,
          0.715239405632019,
          -1.0403629541397095,
          1.7626842260360718,
          -0.5126031637191772,
          1.5470149517059326,
          1.4819207191467285,
          0.2146521806716919,
          0.2650880813598633,
          -0.33259478211402893,
          -0.679353654384613,
          0.33573517203330994,
          0.35723137855529785,
          0.508020281791687,
          0.5398073196411133,
          1.4938474893569946,
          1.0282858610153198,
          1.1348516941070557,
          1.7660857439041138,
          0.37673473358154297,
          0.501476526260376,
          -1.8839468955993652,
          -0.679176390171051,
          0.8696630001068115,
          -1.557816743850708,
          0.5185440182685852,
          -0.3255554437637329,
          0.07825963944196701,
          -1.140786051750183
         ],
         "y": [
          -1.5817906856536865,
          -0.06447271257638931,
          -0.4997028112411499,
          0.18540142476558685,
          -0.15545958280563354,
          0.11552109569311142,
          -1.0166341066360474,
          0.5748610496520996,
          1.4196159839630127,
          0.5820928812026978,
          0.07508774846792221,
          -0.25362226366996765,
          0.13412971794605255,
          -0.7366355657577515,
          -0.04526667296886444,
          0.35875582695007324,
          0.8714503645896912,
          0.14397063851356506,
          -0.7080085277557373,
          -0.1352548599243164,
          1.2945066690444946,
          2.350609302520752,
          1.209783911705017,
          -0.43664830923080444,
          1.7699939012527466,
          1.4439457654953003,
          -0.3363352119922638,
          1.9269694089889526,
          1.121272087097168,
          -0.18200084567070007,
          -0.9922037720680237,
          -0.2818024456501007,
          -0.12164212763309479,
          -0.4977646470069885,
          0.2834306061267853,
          0.0036269279662519693,
          0.043667443096637726,
          -1.6096223592758179,
          0.19635887444019318,
          -1.3924607038497925
         ],
         "z": [
          -3.1462442874908447,
          0.8430724740028381,
          0.381350576877594,
          2.088820457458496,
          0.663757860660553,
          0.6578980088233948,
          0.1910519003868103,
          -0.14419132471084595,
          0.5574296712875366,
          -0.03126606345176697,
          -0.3774397373199463,
          0.4863843619823456,
          0.2918457090854645,
          0.035693030804395676,
          -0.19451861083507538,
          1.6789932250976562,
          -0.6975589990615845,
          0.49800848960876465,
          0.28139767050743103,
          1.101819634437561,
          -0.5366460680961609,
          0.5473771095275879,
          0.41110414266586304,
          1.2415246963500977,
          -1.6986762285232544,
          0.5712112784385681,
          0.3098830580711365,
          -0.6243330240249634,
          0.03267443925142288,
          1.047691822052002,
          0.5042922496795654,
          1.5881681442260742,
          -1.3440923690795898,
          0.06713423132896423,
          0.2674228250980377,
          -1.0396760702133179,
          0.39683014154434204,
          -0.5754148960113525,
          -0.06474851071834564,
          -0.8671362996101379
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 3
         },
         "mode": "markers",
         "name": "Output Points",
         "type": "scatter3d",
         "x": [
          0.9659568071365356,
          -0.7929197549819946,
          -1.2957332134246826,
          -0.8116967082023621,
          1.2473053932189941,
          1.248598575592041,
          0.46873772144317627,
          0.06181380897760391,
          1.0643484592437744,
          1.3870925903320312,
          -0.7579498291015625,
          0.012641377747058868,
          0.4643729329109192,
          -0.9584178328514099,
          -0.2418847382068634,
          -0.32254183292388916,
          0.2466886043548584,
          0.8695996999740601,
          1.0761148929595947,
          -0.514194667339325,
          0.9913893938064575,
          0.5660960674285889,
          -0.1510278284549713,
          1.5836527347564697,
          1.5395665168762207,
          -0.7990862131118774,
          -0.5478795766830444,
          0.626538872718811,
          -1.3947551250457764,
          -0.21800482273101807,
          -0.4694807827472687,
          -0.9352852702140808,
          -0.5876941680908203,
          -0.1610567569732666,
          0.19197773933410645,
          -1.5103578567504883,
          0.5211918950080872,
          -1.222267746925354,
          0.2949925363063812,
          -0.9635084271430969
         ],
         "y": [
          0.5196952223777771,
          -0.9081082940101624,
          0.9797762632369995,
          0.49329233169555664,
          -0.5511630773544312,
          -0.6946274042129517,
          0.858916699886322,
          -0.4500449299812317,
          -0.12325907498598099,
          -0.3682214617729187,
          0.9763132333755493,
          1.9342185258865356,
          0.40430593490600586,
          -0.6433956623077393,
          -0.7475595474243164,
          -1.9410154819488525,
          -0.8697607517242432,
          0.3523399233818054,
          0.9380859136581421,
          -1.4644712209701538,
          -0.4917811453342438,
          -0.23001447319984436,
          0.11008047312498093,
          0.27363336086273193,
          0.6489107012748718,
          -1.6126282215118408,
          -0.15205423533916473,
          -0.1282481551170349,
          -1.2882921695709229,
          -1.7785930633544922,
          -1.6369625329971313,
          0.8032191395759583,
          2.156639575958252,
          1.4229871034622192,
          0.35214707255363464,
          -0.45382556319236755,
          0.8275479078292847,
          -1.3742400407791138,
          1.0413291454315186,
          1.253407597541809
         ],
         "z": [
          0.7045953273773193,
          -0.06341565400362015,
          -0.7221665382385254,
          1.002689003944397,
          0.45429089665412903,
          -0.06681852787733078,
          0.7699422836303711,
          0.08822281658649445,
          0.22920432686805725,
          0.14676472544670105,
          -0.1749531626701355,
          -0.49611660838127136,
          0.5745284557342529,
          -0.14361882209777832,
          0.009085901081562042,
          -0.8436906933784485,
          0.3941337466239929,
          1.3709806203842163,
          -0.048697687685489655,
          -0.6567091345787048,
          0.541388213634491,
          0.71986985206604,
          0.5334759950637817,
          -0.12915188074111938,
          -0.10779114812612534,
          -1.1462042331695557,
          0.15025824308395386,
          0.3842454254627228,
          -2.34692645072937,
          -0.48188552260398865,
          -0.6232878565788269,
          -0.2944979965686798,
          -0.747646152973175,
          -0.264669805765152,
          1.3432021141052246,
          -2.7280325889587402,
          0.5504963397979736,
          -1.355186939239502,
          1.0365190505981445,
          -0.5918249487876892
         ]
        }
       ],
       "layout": {
        "scene": {
         "xaxis": {
          "title": {
           "text": "X-Dimension"
          }
         },
         "yaxis": {
          "title": {
           "text": "Y-Dimension"
          }
         },
         "zaxis": {
          "title": {
           "text": "Z-Dimension"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Input and Output Points in 3D Space"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_output_visualization(losses[\"labels\"], duration=4, randomseed=False, output=True, output_data=losses[\"outputs\"])\n",
    "interactive_output_visualization(losses[\"labels_t\"], duration=4, randomseed=False, output=True, output_data=losses[\"outputs_t\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove later \\\\ For manual observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.6580392  -0.14665686 -0.08613642]\n",
      " [-0.25062385  0.7725525  -0.508908  ]\n",
      " [-1.5131649   0.213537    0.20239104]\n",
      " ...\n",
      " [-0.46813944 -0.30805194  0.04987756]\n",
      " [-1.6501478  -0.15389913 -0.52092695]\n",
      " [-0.84675527  1.003554   -0.55212957]]\n",
      "\n",
      "[[ 0.95444053  0.28293216  0.05500186]\n",
      " [-0.32295996  0.65617037 -0.6964584 ]\n",
      " [-0.93982553  0.6268065   1.3433021 ]\n",
      " ...\n",
      " [-0.1899628  -0.5016459  -0.10208972]\n",
      " [-1.493985   -0.11434855  0.40569332]\n",
      " [-0.9866807   0.8408753  -0.79803693]]\n"
     ]
    }
   ],
   "source": [
    "print(losses[\"outputs_t\"])\n",
    "print()\n",
    "print(losses[\"labels_t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8771036   0.7248149   0.3721023 ]\n",
      " [-0.85428506  0.70963234  0.39909413]\n",
      " [-0.82435125  0.7074326   0.4372099 ]\n",
      " ...\n",
      " [ 1.5298764   0.18830186 -0.13311768]\n",
      " [ 1.536002    0.17626634 -0.14575046]\n",
      " [ 1.576775    0.30497426 -0.07926943]]\n",
      "\n",
      "[[-0.51455855  0.6345319   0.6635679 ]\n",
      " [-0.49432316  0.63156945  0.6758016 ]\n",
      " [-0.474348    0.6273448   0.6853503 ]\n",
      " ...\n",
      " [ 1.5170327  -0.06560533  0.12764059]\n",
      " [ 1.5168868  -0.06152498  0.12844376]\n",
      " [ 1.5165348  -0.05362112  0.12474921]]\n"
     ]
    }
   ],
   "source": [
    "print(losses[\"outputs\"])\n",
    "print()\n",
    "print(losses[\"labels\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
