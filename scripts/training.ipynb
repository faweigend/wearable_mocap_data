{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Prediction From Smartwatch Data\n",
    "### Part 2\n",
    "\n",
    "Welcome, this notebook is the second chapter of the hackathon challenge to predict human arm pose from the sensor data of a single smartwatch. In other words, a human wears a smartwatch, and we will try to predict their arm pose from the sensor outputs we can read from the watch.\n",
    "\n",
    "## Creating a Predictive Model with Machine Learning\n",
    "\n",
    "In this second chapter, we will have the opportunity to utilize the processed data from the previous chapter and train a basic Neural Network to generate predictions. The following sections will guide you step-by-step for model architecture design, model training and evaluation, performance analysis and 3D visualization for better interpretability of the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture Design:\n",
    "The notebook includes the design and definition of the neural network architecture. Utilizing the popular PyTorch library, the model's architecture is structured, comprising input, two hidden, and output layers. The choice of activation functions, loss functions, and optimization algorithms is also made in this section.\n",
    "\n",
    "Model Training:\n",
    "The neural network is trained using the preprocessed dataset. The notebook implements the training loop, where batches of data are fed into the model iteratively. During training, the model learns to optimize its parameters to minimize the defined loss function through backpropagation. Training hyperparameters, such as learning rate, batch size, and number of epochs, may be fine-tuned to achieve optimal performance.\n",
    "\n",
    "Model Evaluation:\n",
    "Following training, the notebook evaluates the neural network's performance on a separate test dataset to assess its generalization ability. The option to define an loss criterion is also implemented in this section to gauge the model's effectiveness in correctly predicting unseen data.\n",
    "\n",
    "Performance Analysis:\n",
    "The notebook visualizes the model's training and validation curves to analyze its learning progress and detect signs of overfitting or underfitting. Analyzing these curves aids in fine-tuning the model's architecture or regularization techniques to achieve a better balance between accuracy and generalization.\n",
    "\n",
    "3D Visualization:\n",
    "Since we will be using the neural networks to estimate the X, Y & Z coordinates, a 3D visualization is presented for the outputs and actual labels to gain a deeper understanding of model's performance. The notebook leverages libraries like Plotly for interactive 3D visualizations.\n",
    "\n",
    "Conclusion and Next Steps:\n",
    "The notebook concludes with a summary of the neural network's performance and insights gained during the training process. Possible avenues for improvement, such as hyperparameter tuning, advanced architectures, or using pre-trained models, are suggested for future exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Downloading Dependencies (Optional)\n",
    "\n",
    "We begin by downloading the libraries in this notebook. In case you have all of these pre-installed, you may skip this step.\n",
    "\n",
    "Note: Some packages were already installed in the first notebook. In case you skipped running the previous notebook, please run step 0 from the first notebook and then you may continute with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries\n",
    "\n",
    "Lets import the libraries we will be primarily working with in this notebook. You may notice some similar ones. Let us help you understand why:\n",
    "- Libraries like Numpy and Pandas are extremely helpful for mathematical functions and data frameworks respectively. Therefore, you would see them almost everywhere.\n",
    "- Matplotlib is often used to display plots and using plotly, we will display interactive plots. But why do we need plots for training a machine learning model? You will find out soon. \n",
    "- Finally, Torch refers to PyTorch that provides powerful tools to deploy Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Libraries for Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split    # Scikit-Lean for preparing the train-test data split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# adjust data paths in config.py\n",
    "import config       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plotting Functions\n",
    "Lets get the simpler stuff out of the way first. Since the neural networks will help to estimate the X, Y & Z coordinates, we use a similar 3D visualization for representing the outputs by the model and actual labels to gain a deeper understanding of model's performance. \n",
    "\n",
    "Secondly, during the training process you will notice the loss decreases with epochs. It is useful to visualize the loss for a number of reasons:\n",
    "- Overfitting: When model starts remembering the training dataset instead of figuring out a \"pattern\".\n",
    "- Performance: To figure out if further training decreases or increases loss \n",
    "\n",
    "Note: Epoch refers to the iteration number where the model goes through the entire training data. For instance, 1 epoch indicates that model has passed through the data once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This section represents the function and helper function required for interactive 3D visualization:\n",
    "        - The interactive_output_visualization function helps in visualizing the X, Y, Z coordinates in an interactive 3D map. It is also used to display the output and original labels\n",
    "\"\"\"\n",
    "\n",
    "# Helper function that provides appropriate the approriate length for columns. \n",
    "# The duration variable helps in defining the length of continuous data - this helps in visualizing a smaller hand trajectory \n",
    "# The randomseed variable helps to visualize the same data without randomizing the selection process\n",
    "# The remaining functions follow the same variables and analogy\n",
    "def data_visualization(data, duration, randomseed=False, output=False):\n",
    "    if output is True:\n",
    "        dim1, dim2, dim3 = data[:,0], data[:,1], data[:,2]\n",
    "    else:\n",
    "        dim1, dim2, dim3 = np.array(data['gt_hand_orig_rua_x']), np.array(data['gt_hand_orig_rua_y']), np.array(data['gt_hand_orig_rua_z'])\n",
    "\n",
    "    if duration is not None:\n",
    "        if randomseed:\n",
    "            np.random.seed(42)\n",
    "        else: \n",
    "            np.random.seed(None)\n",
    "        reduced_samples = 10 * duration\n",
    "        start_index = np.random.choice(len(dim1) - reduced_samples + 1)\n",
    "        dim1 = dim1[start_index : start_index + reduced_samples]\n",
    "        dim2 = dim2[start_index : start_index + reduced_samples]\n",
    "        dim3 = dim3[start_index : start_index + reduced_samples]\n",
    "        \n",
    "    return dim1, dim2, dim3\n",
    "\n",
    "def interactive_output_visualization(data, duration=None, randomseed=False, output=False, output_data = None):\n",
    "    dim1, dim2, dim3 = data_visualization(data, duration, randomseed, output)\n",
    "    \n",
    "    trace = go.Scatter3d(x=dim1, y=dim2, z=dim3, mode='markers', marker=dict(size=3, \n",
    "                                                                             color='blue', \n",
    "                                                                             opacity=0.8),\n",
    "                                                                             name=\"Input Points\")\n",
    "\n",
    "    if output:\n",
    "        dim1, dim2, dim3 = data_visualization(output_data, duration, randomseed, output)\n",
    "        trace_output = go.Scatter3d(x=dim1, y=dim2, z=dim3, mode='markers', marker=dict(size=3, \n",
    "                                                                                    color='red', \n",
    "                                                                                    opacity=0.5), \n",
    "                                                                                    name='Output Points')\n",
    "        traces = [trace, trace_output]\n",
    "        title = 'Input and Output Points in 3D Space'\n",
    "    else:\n",
    "        traces = [trace]\n",
    "        title = 'Values in 3D Space'\n",
    "\n",
    "    layout = go.Layout(title=title, scene=dict(xaxis=dict(title='X-Dimension'),\n",
    "                                                                        yaxis=dict(title='Y-Dimension'),\n",
    "                                                                        zaxis=dict(title='Z-Dimension')))\n",
    "\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "def lossPlot(losses):\n",
    "    epochs_total = range(1, 1 + len(losses[\"evaluation\"]))\n",
    "\n",
    "    plt.subplots(1, figsize=(20,5))\n",
    "    plt.plot(epochs_total, losses[\"training\"], label = \"Training Loss\")\n",
    "    plt.plot(epochs_total, losses[\"evaluation\"], label = \"Evaluation Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Losses over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Processed / Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = pd.read_csv(\"../data/normalized_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader & Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Data Loader: \n",
    "        This serves as the framework to load data using pandas and segment out the useful columns (labels and inputs)\n",
    "\"\"\"\n",
    "column_nos_features = list(range(23,42))\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__ (self, data):\n",
    "        self.data = data\n",
    "        self.features = self.data.iloc[:, column_nos_features].values     # All smart watch inputs that contribute towards the outputs\n",
    "        self.labels = self.data.iloc[:, 0:3].values          # The model predicts X, Y, Z coordinates of hand\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32) \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return features, label\n",
    "    \n",
    "\"\"\"\n",
    "    Neural Network / Model:\n",
    "        A simple linear architecture based model with 3 layers (including input and output layers). \n",
    "\"\"\"\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__ (self, input_size, hidden_size, output_size):\n",
    "        super (MyModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size + int(hidden_size/2))\n",
    "        self.fc2 = nn.Linear(hidden_size + int(hidden_size/2), hidden_size - int(hidden_size/2))\n",
    "        self.fc3 = nn.Linear(hidden_size - int(hidden_size/2), output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    We encourage you to play around with these parameters to see what works better for this model architecture and dataset.\n",
    "\"\"\"\n",
    "batch_size = 1000        \n",
    "learning_rate = 0.0001\n",
    "num_epochs = 500\n",
    "test_data_size = 0.2            # Range 0-1. You may increase or decrease the recommended testing data size by updating this variable\n",
    "\n",
    "hidden_size = 128               # This represents the middle layer of the neural network. You may manually play around with the model architecture by adding more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (fc1): Linear(in_features=19, out_features=192, bias=True)\n",
       "  (fc2): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loading calls to class and functions\n",
    "dataset = MyDataset(data_normalized)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=test_data_size, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialzing the model with inputs\n",
    "input_size = len(dataset.features[0])\n",
    "output_size = len(dataset.labels[0])\n",
    "\n",
    "model = MyModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()                                        # Try out different loss functions and optimizers to see what works with different tasks. \n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)     \n",
    "\n",
    "\"\"\"\n",
    "    If you have GPU resource available, the training process will be faster!\n",
    "    It's okay if you dont have a GPU. The code works without one as well. \n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on loss functions and optimizers, visit:  \n",
    "https://pytorch.org/docs/stable/nn.html#loss-functions     \n",
    "https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/affanbinusman/Dropbox (ASU)/IRL-Lab/P&G/p_and_g_hackathon/scripts/Training.ipynb Cell 14\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/affanbinusman/Dropbox%20%28ASU%29/IRL-Lab/P%26G/p_and_g_hackathon/scripts/Training.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()                   \u001b[39m# Zeros the optimizer before generating output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/affanbinusman/Dropbox%20%28ASU%29/IRL-Lab/P%26G/p_and_g_hackathon/scripts/Training.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)                 \u001b[39m# Calculates the output\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/affanbinusman/Dropbox%20%28ASU%29/IRL-Lab/P%26G/p_and_g_hackathon/scripts/Training.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)       \u001b[39m# Finds loss as per the criteria defined\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/affanbinusman/Dropbox%20%28ASU%29/IRL-Lab/P%26G/p_and_g_hackathon/scripts/Training.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()                         \u001b[39m# Back propogation of loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/affanbinusman/Dropbox%20%28ASU%29/IRL-Lab/P%26G/p_and_g_hackathon/scripts/Training.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()                        \u001b[39m# Updates parameters based on gradients computed duing back propogation\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:3098\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3096\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3098\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "# For visually observing the losses\n",
    "losses = {\"training\" : [], \n",
    "          \"evaluation\" : [], \n",
    "          \"labels\" : [], \n",
    "          \"outputs\" : [],\n",
    "          \"labels_t\" : [], \n",
    "          \"outputs_t\" : []\n",
    "          }\n",
    "\n",
    "# Training & Evaluation Porcess\n",
    "patience = 10                      # Patience based stopping criteria. You may also play around with this number for early or delayed stopping of the model from over-fitting\n",
    "best_loss = float('inf')\n",
    "num_epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    all_outputs_training = []\n",
    "    all_labels_training = []\n",
    "    model.train(True)\n",
    "    running_loss = 0.0                          # To calculates the losses in training for each epoch\n",
    "    for inputs, labels in train_loader:\n",
    "        # Transfers data to GPU/CPU\n",
    "        inputs = inputs.to(device)              \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()                   # Zeros the optimizer before generating output\n",
    "        outputs = model(inputs)                 # Calculates the output\n",
    "        loss = criterion(outputs, labels)       # Finds loss as per the criteria defined\n",
    "        loss.backward()                         # Back propogation of loss\n",
    "        optimizer.step()                        # Updates parameters based on gradients computed duing back propogation\n",
    "        \n",
    "        running_loss += loss.item()             # Calculates loss over the training\n",
    "\n",
    "        all_outputs_training.append(outputs.detach().numpy())  # Append outputs to the list\n",
    "        all_labels_training.append(labels.detach().numpy())    # Append labels to the list\n",
    "    \n",
    "    training_loss = running_loss/len(train_loader)\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluating the model that has been trained (so far). \n",
    "    You would notice similar steps as during the training process. The lack of a few lines of code is because we \n",
    "    are evaluating the model here and not training it.\n",
    "    \"\"\"\n",
    "    model.train(False)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_outputs.append(outputs.cpu().numpy())  \n",
    "            all_labels.append(labels.cpu().numpy())    \n",
    "                \n",
    "    mean_loss = total_loss / len(test_loader)\n",
    "\n",
    "    if mean_loss < best_loss:\n",
    "        best_loss = mean_loss\n",
    "        num_epochs_without_improvement = 0\n",
    "    else:\n",
    "        num_epochs_without_improvement += 1\n",
    "        if num_epochs_without_improvement == patience:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "    \n",
    "    # Recording the data values for\n",
    "    losses[\"training\"].append(training_loss)\n",
    "    losses[\"evaluation\"].append(mean_loss)\n",
    "    losses[\"outputs\"] = np.concatenate(all_outputs)\n",
    "    losses[\"labels\"] = np.concatenate(all_labels)\n",
    "    \n",
    "    losses[\"outputs_t\"] = np.concatenate(all_outputs_training)\n",
    "    losses[\"labels_t\"] = np.concatenate(all_labels_training)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {training_loss}, Smooth L1 Loss: {mean_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossPlot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_total = range(1, 1 + len(losses[\"evaluation\"]))\n",
    "\n",
    "plt.subplots(1, figsize=(20,5))\n",
    "plt.plot(epochs_total, losses[\"training\"], label = \"Training Loss\")\n",
    "plt.plot(epochs_total, losses[\"evaluation\"], label = \"Evaluation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_output_visualization(losses[\"labels\"], duration=4, randomseed=False, output=True, output_data=losses[\"outputs\"])\n",
    "interactive_output_visualization(losses[\"labels_t\"], duration=4, randomseed=False, output=True, output_data=losses[\"outputs_t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses[\"outputs_t\"])\n",
    "print()\n",
    "print(losses[\"labels_t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses[\"outputs\"])\n",
    "print()\n",
    "print(losses[\"labels\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
